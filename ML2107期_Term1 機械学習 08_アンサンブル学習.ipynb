{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7ba312d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "X = df[[\"GrLivArea\", \"YearBuilt\"]].values\n",
    "y = df[[\"SalePrice\"]].values.flatten()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# 標準化\n",
    "sc = StandardScaler()\n",
    "tr_X_train = sc.fit_transform(X_train)\n",
    "tr_X_test = sc.fit_transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339f9ae2",
   "metadata": {},
   "source": [
    "### 【問題1】ブレンディングのスクラッチ実装\n",
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。精度があがるとは、検証データに対する平均二乗誤差（MSE）が小さくなることを指します。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc4f9b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形回帰による平均二乗誤差 : 2850683002.96796\n",
      "SVRによる平均二乗誤差      : 7054588160.53784\n",
      "決定木による平均二乗誤差   : 3274901353.14446\n",
      "ブレンディング : 5374735716.99173\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# 線形回帰による分析\n",
    "lm = LinearRegression().fit(tr_X_train, y_train)\n",
    "lm_pred = lm.predict(tr_X_test)\n",
    "lm_mean_squared_error = mean_squared_error(y_test, lm_pred)\n",
    "print(\"線形回帰による平均二乗誤差 : {:.5f}\".format(lm_mean_squared_error))\n",
    "\n",
    "# SVR\n",
    "svr= SVR(C=1.0, epsilon=0.2).fit(tr_X_train, y_train)\n",
    "svr_pred = svr.predict(tr_X_test)\n",
    "svr_mean_squared_error = mean_squared_error(y_test, svr_pred)\n",
    "print(\"SVRによる平均二乗誤差      : {:.5f}\".format(svr_mean_squared_error))\n",
    "\n",
    "# 決定木\n",
    "dtr = DecisionTreeRegressor(max_depth=3).fit(tr_X_train, y_train)\n",
    "dtr_pred = dtr.predict(tr_X_test)\n",
    "dtr_mean_squared_error = mean_squared_error(y_test, dtr_pred)\n",
    "print(\"決定木による平均二乗誤差   : {:.5f}\".format(dtr_mean_squared_error))\n",
    "\n",
    "\n",
    "# ブレンディング\n",
    "mean_pred = (0.1 * lm_pred) + (0.9 * svr_pred) + (0.1 * dtr_pred)\n",
    "print(\"ブレンディング : {:.5f}\".format(mean_squared_error(y_test, mean_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aec37b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "線形回帰による平均二乗誤差           : 2850683002.96796\n",
      "決定木による平均二乗誤差             : 3274901353.14446\n",
      "ランダムフォレストによる平均二乗誤差 : 3451492508.72350\n",
      "ブレンディング : 3015405890.93751\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# 線形回帰による分析\n",
    "lm = LinearRegression().fit(tr_X_train, y_train)\n",
    "lm_pred = lm.predict(tr_X_test)\n",
    "lm_mean_squared_error = mean_squared_error(y_test, lm_pred)\n",
    "print(\"線形回帰による平均二乗誤差           : {:.5f}\".format(lm_mean_squared_error))\n",
    "\n",
    "# 決定木\n",
    "dtr = DecisionTreeRegressor(max_depth=3).fit(tr_X_train, y_train)\n",
    "dtr_pred = dtr.predict(tr_X_test)\n",
    "dtr_mean_squared_error = mean_squared_error(y_test, dtr_pred)\n",
    "print(\"決定木による平均二乗誤差             : {:.5f}\".format(dtr_mean_squared_error))\n",
    "\n",
    "# ランダムフォレスト\n",
    "rfr = RandomForestRegressor(max_depth=2, random_state=0).fit(tr_X_train, y_train)\n",
    "rfr_pred = rfr.predict(tr_X_test)\n",
    "rfr_mean_squared_error = mean_squared_error(y_test, rfr_pred)\n",
    "print(\"ランダムフォレストによる平均二乗誤差 : {:.5f}\".format(rfr_mean_squared_error))\n",
    "\n",
    "\n",
    "# ブレンディング\n",
    "mean_pred = (0.2 * lm_pred) + (0.3 * dtr_pred) + (0.5 * rfr_pred)\n",
    "print(\"ブレンディング : {:.5f}\".format(mean_squared_error(y_test, mean_pred)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d81333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVRによる平均二乗誤差                : 7054588160.53784\n",
      "決定木による平均二乗誤差             : 3274901353.14446\n",
      "ランダムフォレストによる平均二乗誤差 : 3451492508.72350\n",
      "ブレンディング : 4829063214.17070\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "\n",
    "# SVR\n",
    "svr= SVR(C=1.0, epsilon=0.2).fit(tr_X_train, y_train)\n",
    "svr_pred = svr.predict(tr_X_test)\n",
    "svr_mean_squared_error = mean_squared_error(y_test, svr_pred)\n",
    "print(\"SVRによる平均二乗誤差                : {:.5f}\".format(svr_mean_squared_error))\n",
    "\n",
    "# 決定木\n",
    "dtr = DecisionTreeRegressor(max_depth=3).fit(tr_X_train, y_train)\n",
    "dtr_pred = dtr.predict(tr_X_test)\n",
    "dtr_mean_squared_error = mean_squared_error(y_test, dtr_pred)\n",
    "print(\"決定木による平均二乗誤差             : {:.5f}\".format(dtr_mean_squared_error))\n",
    "\n",
    "# ランダムフォレスト\n",
    "rfr = RandomForestRegressor(max_depth=2, random_state=0).fit(tr_X_train, y_train)\n",
    "rfr_pred = rfr.predict(tr_X_test)\n",
    "rfr_mean_squared_error = mean_squared_error(y_test, rfr_pred)\n",
    "print(\"ランダムフォレストによる平均二乗誤差 : {:.5f}\".format(rfr_mean_squared_error))\n",
    "\n",
    "\n",
    "# ブレンディング\n",
    "mean_pred = (0.6 * svr_pred) + (0.2 * dtr_pred) + (0.2 * rfr_pred)\n",
    "print(\"ブレンディング : {:.5f}\".format(mean_squared_error(y_test, mean_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7f7c3",
   "metadata": {},
   "source": [
    "### 【問題2】バギングのスクラッチ実装\n",
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。<br>\n",
    "\n",
    "### バギングとは\n",
    "バギングは入力データの選び方を多様化する方法です。訓練データから重複を許した上でランダムに抜き出すことで、N種類のサブセット（ ブートストラップサンプル ）を作り出します。それらによってモデルをN個学習し、推定結果の平均をとります。ブレンディングと異なり、それぞれの重み付けを変えることはありません。<br>\n",
    "<a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">sklearn.model_selection.train_test_split — scikit-learn 0.21.3 documentation</a><br>\n",
    "推定結果の平均をとる部分はブレンディングと同様の実装になります。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7215d627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bagging():\n",
    "    def __init__(self):\n",
    "        self.model_list = []\n",
    "        self.sc = StandardScaler()\n",
    "\n",
    "    def fit(self, models, X, y):\n",
    "        for boost_num, model in enumerate(models):\n",
    "            bst_X_train, bst_X_test, bst_y_train, bst_y_test = train_test_split(X, y, test_size=0.2, random_state=boost_num, shuffle=True)\n",
    "            tr_X_train = self.sc.fit_transform(bst_X_train)\n",
    "            tr_X_test = self.sc.fit_transform(bst_X_test)\n",
    "            model_fit_result = model.fit(bst_X_train, bst_y_train)\n",
    "            self.model_list.append(model_fit_result)\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.pred_data = np.zeros((X.shape[0],len(self.model_list)))\n",
    "        for i, model in enumerate(self.model_list):\n",
    "            self.pred = model.predict(X)\n",
    "            self.pred_data[:,i] = self.pred\n",
    "        self.final_pred = np.mean(self.pred_data, axis=1)\n",
    "        return self.final_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d89aa590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "バギング : 11830182263.32102\n"
     ]
    }
   ],
   "source": [
    "\n",
    "models = [DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=3), DecisionTreeRegressor(max_depth=3)]\n",
    "\n",
    "bg = Bagging()\n",
    "bg.fit(models = models, X = X, y = y)\n",
    "final_pred = bg.predict(tr_X_test)\n",
    "print(\"バギング : {:.5f}\".format(mean_squared_error(y_test, final_pred)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf81e0b",
   "metadata": {},
   "source": [
    "### 【問題3】スタッキングのスクラッチ実装\n",
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。<br>\n",
    "<br>\n",
    "### スタッキングとは\n",
    "スタッキングの手順は以下の通りです。<br>最低限ステージ0とステージ1があればスタッキングは成立するため、それを実装してください。まずは $K_0=3, M_0=2$ 程度にします。<br>\n",
    "#### 《学習時》\n",
    "（ステージ $0$ ）<br>\n",
    "　訓練データを $K_0$ 個に分割する。<br>\n",
    "　分割した内の $(K_0 - 1)$ 個をまとめて訓練データ、残り $1$ 個を推定用データとする組み合わせが $K_0$ 個作れる。<br>\n",
    "　あるモデルのインスタンスを $K_0$ 個用意し、異なる訓練データを使い学習する。<br>\n",
    "　それぞれの学習済みモデルに対して、使っていない残り $1$ 個の推定用データを入力し、推定値を得る。（これをブレンドデータと呼ぶ）<br>\n",
    "　さらに、異なるモデルのインスタンスも $K_0$ 個用意し、同様のことを行う。モデルが $M_0$ 個あれば、 $M_0$ 個のブレンドデータが得られる。<br>\n",
    "<br>\n",
    "（ステージ $n$ ）<br>\n",
    "　ステージ $n-1$ のブレンドデータを$M_{n-1}$ 次元の特徴量を持つ訓練データと考え、 $K_n$ 個に分割する。以下同様である。<br>\n",
    "<br>\n",
    "（ステージ $N$ ）＊最後のステージ<br>\n",
    "　ステージ $N-1$ の $M_{N-1}$ 個のブレンドデータを$M_{N-1}$ 次元の特徴量の入力として、1種類のモデルの学習を行う。これが最終的な推定を行うモデルとなる。<br>\n",
    "#### 《推定時》\n",
    "（ステージ $0$ ）<br>\n",
    "　テストデータを $K_0×M_0$ 個の学習済みモデルに入力し、$K_0×M_0$ 個の推定値を得る。これを $K_0$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）<br>\n",
    "<br>\n",
    "（ステージ $n$ ）<br>\n",
    "　ステージ $n-1$ で得たブレンドテストを $K_n×M_n$ 個の学習済みモデルに入力し、$K_n×M_n$ 個の推定値を得る。これを $K_n$ の軸で平均値を求め $M_0$ 次元の特徴量を持つデータを得る。（ブレンドテストと呼ぶ）<br>\n",
    "<br>\n",
    "（ステージ $N$ ）＊最後のステージ<br>\n",
    "　ステージ $N-1$ で得たブレンドテストを学習済みモデルに入力し、推定値を得る。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "fd5eec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "class Stacking():\n",
    "\n",
    "    def __init__(self, models, end_model):\n",
    "        self.models = models\n",
    "        self.end_model = end_model\n",
    "\n",
    "        self.brend_pred = np.array([]) # X_test + ブレンドデータ（配列）\n",
    "        self.X_test_brend = np.array([]) # X_test + ブレンドデータ（配列）\n",
    "        self.brend_data = np.array([]) # end_modelを検証するための特徴量\n",
    "\n",
    "        self.brend_preds = np.array([]) # 検証結果格納用\n",
    "        self.model_list = [] # モデル格納用\n",
    "\n",
    "    def fit(self, X, y, k, random_num):\n",
    "        self.k = k\n",
    "        self.fit_models = []\n",
    "        kf = KFold(n_splits=self.k, random_state=random_num, shuffle=True)\n",
    "        # モデル数分ループ\n",
    "        _i = 0\n",
    "        for model in self.models:\n",
    "            # 再分割(※)する訓練データ数分ループ　※本データ→訓練、検証の２つに分け、訓練データを更に訓練、検証に分ける\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                # predictでの検証用に学習モデルを保存しておく\n",
    "                self.fit_models.append(model.fit(X[train_index], y[train_index]))\n",
    "                # ブレンドデータ\n",
    "                y_pred = model.predict(X[test_index])\n",
    "                # ブレンドデータを新たな特徴量として扱う\n",
    "                self.brend_pred = np.append(self.brend_pred, y_pred)\n",
    "            if _i == 0:\n",
    "                self.X_test_brend = np.append(self.X_test_brend, self.brend_pred.reshape(self.brend_pred.shape[0], 1))\n",
    "            else :\n",
    "                self.X_test_brend = np.append(self.X_test_brend.reshape(self.X_test_brend.shape[0], 1), self.brend_pred.reshape(self.brend_pred.shape[0], 1), axis = 1)\n",
    "            # モデルの検証ごとにデータ(pred)を分けたいため、初期化\n",
    "            self.brend_pred = np.array([])\n",
    "            _i += 1\n",
    "        print(\"self.X_test_brend:{}\".format(self.X_test_brend.shape))\n",
    "        self.end_model.fit(self.X_test_brend, y)\n",
    "\n",
    "    def predict(self, X):\n",
    "#         self.pred_data = np.zeros((X.shape[0],self.k))\n",
    "        self.brend_pred = np.array([])\n",
    "        mean_pred = np.array([])\n",
    "        # モデル数分ループ\n",
    "        i = 1\n",
    "        for model in self.fit_models:\n",
    "            # ブレンドデータ\n",
    "            y_pred = model.predict(X)\n",
    "            if (i == 1) or (i % self.k == 1) :\n",
    "                self.brend_pred = np.append(self.brend_pred, y_pred.reshape(y_pred.shape[0], 1))\n",
    "                self.brend_pred = self.brend_pred.reshape(y_pred.shape[0], 1)\n",
    "            else :\n",
    "                self.brend_pred = np.append(self.brend_pred, y_pred.reshape(y_pred.shape[0], 1), axis = 1)\n",
    "                # モデルデータ数(モデル数×ループ回数) / モデル数 をすることで、モデル事にデータの平均が取れる\n",
    "                # モデル毎に平均を取る→ndarray(X.shape[0]、モデル数（＝X.shape[1])）とする\n",
    "                if (i % self.k) == 0:\n",
    "                    # if文に入って最初の１回目　ndarrayへ綺麗に追加できないので…\n",
    "                    if (i / self.k) == 1:\n",
    "                        mean_pred = np.mean(self.brend_pred, axis = 1)\n",
    "                        mean_pred = mean_pred.reshape(mean_pred.shape[0], 1)\n",
    "                        self.brend_pred = np.array([])\n",
    "                    else:\n",
    "                        mean_pred = np.append(mean_pred, np.mean(self.brend_pred, axis = 1).reshape(y_pred.shape[0], 1), axis = 1)\n",
    "            i += 1\n",
    "\n",
    "        # end_modelにてブレンドデータをpredict\n",
    "        end_pred = self.end_model.predict(mean_pred)\n",
    "\n",
    "        return end_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "23bd3825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.X_test_brend:(1168, 2)\n",
      "(292,)\n",
      "スタッキング :  7077222322.083442\n"
     ]
    }
   ],
   "source": [
    "models = [LinearRegression(), DecisionTreeRegressor(max_depth=3)]\n",
    "\n",
    "sk = Stacking(models, end_model=SVR(C=1.0, epsilon=0.2))\n",
    "sk.fit(X=tr_X_train, y=y_train, k=3, random_num=42)\n",
    "end_pred = sk.predict(X_test)\n",
    "print(end_pred.shape)\n",
    "print(\"スタッキング : \", mean_squared_error(y_test, end_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc3fe3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

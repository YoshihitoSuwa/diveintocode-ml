{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "708b00ee",
   "metadata": {},
   "source": [
    "### 【問題1】全結合層のクラス化\n",
    "全結合層のクラス化を行なってください。<br>\n",
    "\n",
    "### 【問題2】初期化方法のクラス化\n",
    "初期化を行うコードをクラス化してください。<br>\n",
    "\n",
    "### 【問題3】最適化手法のクラス化\n",
    "最適化手法のクラス化を行なってください。<br>\n",
    "\n",
    "### 【問題4】活性化関数のクラス化\n",
    "活性化関数のクラス化を行なってください。<br>\n",
    "\n",
    "### 【問題5】ReLUクラスの作成\n",
    "現在一般的に使われている活性化関数であるReLU（Rectified Linear Unit）をReLUクラスとして実装してください。<br>\n",
    "\n",
    "### 【問題6】重みの初期値\n",
    "ここまでは重みやバイアスの初期値は単純にガウス分布で、標準偏差をハイパーパラメータとして扱ってきました。しかし、どのような値にすると良いかが知られています。シグモイド関数やハイパボリックタンジェント関数のときは Xavierの初期値 （またはGlorotの初期値）、ReLUのときは Heの初期値 が使われます。<br>\n",
    "XavierInitializerクラスと、HeInitializerクラスを作成してください。<br>\n",
    "\n",
    "### 【問題7】最適化手法\n",
    "学習率は学習過程で変化させていく方法が一般的です。基本的な手法である AdaGrad のクラスを作成してください。<br>\n",
    "\n",
    "### 【問題8】クラスの完成\n",
    "任意の構成で学習と推定が行えるScratchDeepNeuralNetrowkClassifierクラスを完成させてください。<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02bdf136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数による計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        # ハイボリックタンジェント関数\n",
    "        Z = np.tanh(self.A)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z):\n",
    "        \"\"\"\n",
    "        バックプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        A = Z * (1 - np.tanh(self.A) ** 2)\n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU関数による計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        # ハイボリックタンジェント関数\n",
    "        Z = np.maximum(0, self.A)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z):\n",
    "        \"\"\"\n",
    "        バックプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        A = Z * (1 - np.maximum(0, self.A) ** 2)\n",
    "\n",
    "        return A\n",
    "\n",
    "    \n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    ソフトマックス関数による計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        # ソフトマックス関数\n",
    "        Z = (np.exp(A) / np.sum(np.exp(A)))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z, Y):\n",
    "        \"\"\"\n",
    "        バックプロバゲーション時の計算＋交差エントロピー誤差\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Y : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "\n",
    "        # ソフトマックス関数\n",
    "        A = Z - Y\n",
    "\n",
    "        # エラーを起こさないための微小値\n",
    "        delta = 1e-7\n",
    "        # 交差エントロピー誤差　←　毎回計算する必要ある？？？\n",
    "        self.L = - np.sum(Y * np.log(Z + delta)) / len(Y)\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - (self.lr * layer.dW)\n",
    "        layer.B = layer.B - (self.lr * layer.dB)\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGradによる学習率の更新\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hW = 0\n",
    "        self.hB = 0\n",
    "#         self.hW = None\n",
    "#         self.hB = None\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # エラーを起こさないための微小値\n",
    "        delta = 1e-7\n",
    "#         if self.hW is None:\n",
    "#             self.hW = layer.dW ** 2\n",
    "#             self.hB = layer.dB ** 2\n",
    "#         else:\n",
    "#             self.hW += layer.dW ** 2\n",
    "#             self.hB += layer.dB ** 2\n",
    "        self.hW += layer.dW ** 2\n",
    "        self.hB += layer.dB ** 2\n",
    "\n",
    "        layer.W = layer.W - (self.lr * layer.dW / (np.sqrt(self.hW) + delta))\n",
    "        layer.B = layer.B - (self.lr * layer.dW / (np.sqrt(self.hB) + delta))\n",
    "\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = n_nodes2\n",
    "\n",
    "        return B\n",
    "\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        # Heによる重みの初期化\n",
    "        W = np.sqrt(2 / n_nodes1) * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = n_nodes2\n",
    "\n",
    "        return B\n",
    "\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # 初期化\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X(or Z) : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.Z = X\n",
    "        A = X @ self.W + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 重みに対する勾配\n",
    "        self.dW = self.Z.T @ dA\n",
    "        dZ = dA @ self.dW.T\n",
    "\n",
    "        # バイアス項に対する勾配\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dZ\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "#         self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier():\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma, batch_size, n_features, n_nodes1, n_nodes2, n_output, num_iter=10, lr=0.0005, verbose = False):\n",
    "        self.sigma = sigma # ガウス分布（正規分布）用の標準偏差\n",
    "        self.lr = lr # 学習率\n",
    "\n",
    "        self.batch_size = batch_size # バッチサイズ\n",
    "        self.n_features = n_features # 特徴量の数\n",
    "        self.n_nodes1 = n_nodes1 # 1層目のノード数\n",
    "        self.n_nodes2 = n_nodes2 # 2層目のノード数\n",
    "        self.n_output = n_output # 出力のクラス数（3層目のノード数）\n",
    "        self.num_iter = num_iter # イテレーション回数\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, fp=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"特徴量ベクトル(batch_size, n_features)：{}\".format(X.shape))\n",
    "        print(\"バッチサイズ(self.batch_size) : {}\".format(self.batch_size)) # バッチサイズ\n",
    "        print(\"特徴量の数(self.n_features) : {}\".format(self.n_features)) # 特徴量の数\n",
    "        print(\"1層目のノード数(self.n_nodes1) : {}\".format(self.n_nodes1)) # 1層目のノード数\n",
    "        print(\"2層目のノード数(self.n_nodes2) : {}\".format(self.n_nodes2)) # 2層目のノード数\n",
    "        print(\"出力のクラス数（3層目のノード数）(self.n_output): {}\".format(self.n_output)) # 出力のクラス数（3層目のノード数）\n",
    "\n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "\n",
    "        self.cee_array = []\n",
    "\n",
    "        for num_iter in range(self.num_iter):\n",
    "\n",
    "            # 初期化と宣言\n",
    "            optimizer1 = SGD(self.lr)\n",
    "            optimizer2 = SGD(self.lr)\n",
    "            optimizer3 = SGD(self.lr)\n",
    "#             optimizer1 = AdaGrad(self.lr)\n",
    "#             optimizer2 = AdaGrad(self.lr)\n",
    "#             optimizer3 = AdaGrad(self.lr)\n",
    "#             s_initializer_1 = SimpleInitializer(self.sigma)\n",
    "#             s_initializer_2 = SimpleInitializer(self.sigma)\n",
    "#             s_initializer_3 = SimpleInitializer(self.sigma)\n",
    "            h_initializer_1 = HeInitializer()\n",
    "            h_initializer_2 = HeInitializer()\n",
    "            h_initializer_3 = HeInitializer()\n",
    "            self.FC1 = FC(self.n_features, self.n_nodes1, h_initializer_1, optimizer1)\n",
    "            self.activation1 = Tanh()\n",
    "#             self.activation1 = ReLU()\n",
    "            self.FC2 = FC(self.n_nodes1, self.n_nodes2, h_initializer_2, optimizer2)\n",
    "            self.activation2 = Tanh()\n",
    "#             self.activation2 = ReLU()\n",
    "            self.FC3 = FC(self.n_nodes2, self.n_output, h_initializer_3, optimizer3)\n",
    "            self.activation3 = Softmax()\n",
    "\n",
    "            # ミニバッチ処理\n",
    "            get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
    "            # get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                ### フォワードプロバゲーション\n",
    "\n",
    "                ### 1層目\n",
    "                A1 = self.FC1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "\n",
    "                ### 2層目\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "\n",
    "                ### 3層目\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                ### バックプロバゲーション\n",
    "                ### 3層目\n",
    "                dA3 = self.activation3.backward(Z3, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "\n",
    "                ### 2層目\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "\n",
    "                ### 1層目\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "\n",
    "            # 交差エントロピーの記録\n",
    "            self.cee_array.append(self.activation3.L)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        ### フォワードプロバゲーション\n",
    "        ### 1層目\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        ### 2層目\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        ### 3層目\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "\n",
    "        # 最も高い確率(z3内のデータ)を判定\n",
    "        max_Z3_index = np.argmax(Z3)\n",
    "        max_Z3 = Z3.flatten()[max_Z3_index]\n",
    "\n",
    "        return max_Z3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "148367ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量ベクトル(batch_size, n_features)：(48000, 784)\n",
      "バッチサイズ(self.batch_size) : 20\n",
      "特徴量の数(self.n_features) : 784\n",
      "1層目のノード数(self.n_nodes1) : 400\n",
      "2層目のノード数(self.n_nodes2) : 200\n",
      "出力のクラス数（3層目のノード数）(self.n_output): 10\n",
      "3.210146079605473e-05\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "# print(y_train.shape) # (60000,)\n",
    "# print(y_train_one_hot.shape) # (60000, 10)\n",
    "# print(y_train_one_hot.dtype) # float64\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "# print(X_train.shape) # (48000, 784)\n",
    "# print(X_val.shape) # (12000, 784)\n",
    "# print(y_train.shape) # (48000, 10)\n",
    "# print(y_val.shape) # (12000, 10)\n",
    "\n",
    "# クラス宣言時の引数\n",
    "sigma = 0.01 # ガウス分布（正規分布）用の標準偏差\n",
    "batch_size = 20 # バッチサイズ\n",
    "n_features = 784 # 特徴量の数\n",
    "n_nodes1 = 400 # 1層目のノード数\n",
    "n_nodes2 = 200 # 2層目のノード数\n",
    "n_output = 10 # 出力のクラス数（3層目のノード数）\n",
    "num_iter = 5 # イテレーション回数\n",
    "lr = 0.000000001 # 学習率\n",
    "verbose = False\n",
    "\n",
    "# インスタンス化\n",
    "sdnnc = ScratchDeepNeuralNetrowkClassifier(sigma, batch_size, n_features, n_nodes1, n_nodes2, n_output, num_iter, lr, verbose)\n",
    "# fit関数呼び出し\n",
    "sdnnc.fit(X_train, y_train, X_val, y_val)\n",
    "accuracy = sdnnc.predict(X_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2aa80485",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(sdnnc.cee_array)), sdnnc.cee_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c79d9422",
   "metadata": {},
   "source": [
    "### 【問題9】学習と推定\n",
    "層の数や活性化関数を変えたいくつかのネットワークを作成してください。そして、MNISTのデータを学習・推定し、Accuracyを計算してください。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "eccce5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "class Tanh():\n",
    "    \"\"\"\n",
    "    ハイパボリックタンジェント関数による計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        # ハイボリックタンジェント関数\n",
    "        Z = np.tanh(self.A)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z):\n",
    "        \"\"\"\n",
    "        バックプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        A = Z * (1 - np.tanh(self.A) ** 2)\n",
    "\n",
    "        return A\n",
    "\n",
    "\n",
    "class ReLU():\n",
    "    \"\"\"\n",
    "    ReLU関数による計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.A = A\n",
    "        # ハイボリックタンジェント関数\n",
    "        Z = np.maximum(0, self.A)\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z):\n",
    "        \"\"\"\n",
    "        バックプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        A = Z * (1 - np.maximum(0, self.A) ** 2)\n",
    "\n",
    "        return A\n",
    "\n",
    "    \n",
    "class Softmax():\n",
    "    \"\"\"\n",
    "    ソフトマックス関数による計算\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def forward(self, A):\n",
    "        \"\"\"\n",
    "        フォワードプロバゲーション時の計算\n",
    "        Parameters\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        # ソフトマックス関数\n",
    "        Z = (np.exp(A) / np.sum(np.exp(A)))\n",
    "\n",
    "        return Z\n",
    "\n",
    "    def backward(self, Z, Y):\n",
    "        \"\"\"\n",
    "        バックプロバゲーション時の計算＋交差エントロピー誤差\n",
    "        Parameters\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Y : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        Z : 次の形のndarray, shape (batch_size, n_nodes?)\n",
    "            出力\n",
    "        \"\"\"\n",
    "\n",
    "        # ソフトマックス関数\n",
    "        A = Z - Y\n",
    "\n",
    "        # エラーを起こさないための微小値\n",
    "        delta = 1e-7\n",
    "        # 交差エントロピー誤差　←　毎回計算する必要ある？？？\n",
    "        self.L = - np.sum(Y * np.log(Z + delta)) / len(Y)\n",
    "\n",
    "        return Z\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    確率的勾配降下法\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W = layer.W - (self.lr * layer.dW)\n",
    "        layer.B = layer.B - (self.lr * layer.dB)\n",
    "\n",
    "\n",
    "class AdaGrad:\n",
    "    \"\"\"\n",
    "    AdaGradによる学習率の更新\n",
    "    Parameters\n",
    "    ----------\n",
    "    lr : 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr):\n",
    "        self.lr = lr\n",
    "        self.hW = 0\n",
    "        self.hB = 0\n",
    "#         self.hW = None\n",
    "#         self.hB = None\n",
    "\n",
    "    def update(self, layer):\n",
    "        \"\"\"\n",
    "        ある層の重みやバイアスの更新\n",
    "        Parameters\n",
    "        ----------\n",
    "        layer : 更新前の層のインスタンス\n",
    "        \"\"\"\n",
    "        # エラーを起こさないための微小値\n",
    "        delta = 1e-7\n",
    "#         if self.hW is None:\n",
    "#             self.hW = layer.dW ** 2\n",
    "#             self.hB = layer.dB ** 2\n",
    "#         else:\n",
    "#             self.hW += layer.dW ** 2\n",
    "#             self.hB += layer.dB ** 2\n",
    "        self.hW += layer.dW ** 2\n",
    "        self.hB += layer.dB ** 2\n",
    "\n",
    "        layer.W = layer.W - (self.lr * layer.dW / (np.sqrt(self.hW) + delta))\n",
    "        layer.B = layer.B - (self.lr * layer.dW / (np.sqrt(self.hB) + delta))\n",
    "\n",
    "\n",
    "class SimpleInitializer:\n",
    "    \"\"\"\n",
    "    ガウス分布によるシンプルな初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    sigma : float\n",
    "      ガウス分布の標準偏差\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma):\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = n_nodes2\n",
    "\n",
    "        return B\n",
    "\n",
    "\n",
    "class HeInitializer:\n",
    "    \"\"\"\n",
    "    Heによる初期化\n",
    "    Parameters\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"\n",
    "        重みの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes1 : int\n",
    "          前の層のノード数\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        W :\n",
    "        \"\"\"\n",
    "        # Heによる重みの初期化\n",
    "        W = np.sqrt(2 / n_nodes1) * np.random.randn(n_nodes1, n_nodes2)\n",
    "\n",
    "        return W\n",
    "\n",
    "    def B(self, n_nodes2):\n",
    "        \"\"\"\n",
    "        バイアスの初期化\n",
    "        Parameters\n",
    "        ----------\n",
    "        n_nodes2 : int\n",
    "          後の層のノード数\n",
    "        Returns\n",
    "        ----------\n",
    "        B :\n",
    "        \"\"\"\n",
    "        B = n_nodes2\n",
    "\n",
    "        return B\n",
    "\n",
    "\n",
    "class FC:\n",
    "    \"\"\"\n",
    "    ノード数n_nodes1からn_nodes2への全結合層\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_nodes1 : int\n",
    "      前の層のノード数\n",
    "    n_nodes2 : int\n",
    "      後の層のノード数\n",
    "    initializer : 初期化方法のインスタンス\n",
    "    optimizer : 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\n",
    "        # 初期化\n",
    "        self.n_nodes1 = n_nodes1\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # initializerのメソッドを使い、self.Wとself.Bを初期化する\n",
    "        self.W = initializer.W(self.n_nodes1, self.n_nodes2)\n",
    "        self.B = initializer.B(self.n_nodes2)\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        フォワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        X(or Z) : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            入力\n",
    "        Returns\n",
    "        ----------\n",
    "        A : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            出力\n",
    "        \"\"\"\n",
    "        self.Z = X\n",
    "        A = X @ self.W + self.B\n",
    "\n",
    "        return A\n",
    "\n",
    "    def backward(self, dA):\n",
    "        \"\"\"\n",
    "        バックワード\n",
    "        Parameters\n",
    "        ----------\n",
    "        dA : 次の形のndarray, shape (batch_size, n_nodes2)\n",
    "            後ろから流れてきた勾配\n",
    "        Returns\n",
    "        ----------\n",
    "        dZ : 次の形のndarray, shape (batch_size, n_nodes1)\n",
    "            前に流す勾配\n",
    "        \"\"\"\n",
    "        # 重みに対する勾配\n",
    "        self.dW = self.Z.T @ dA\n",
    "        dZ = dA @ self.dW.T\n",
    "\n",
    "        # バイアス項に対する勾配\n",
    "        self.dB = np.sum(dA, axis=0)\n",
    "\n",
    "        # 更新\n",
    "        self = self.optimizer.update(self)\n",
    "\n",
    "        return dZ\n",
    "\n",
    "\n",
    "class GetMiniBatch:\n",
    "    \"\"\"\n",
    "    ミニバッチを取得するイテレータ\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\n",
    "      正解値\n",
    "    batch_size : int\n",
    "      バッチサイズ\n",
    "    seed : int\n",
    "      NumPyの乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "#         self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    def __getitem__(self,item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]        \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "\n",
    "\n",
    "class ScratchDeepNeuralNetrowkClassifier_N():\n",
    "    \"\"\"\n",
    "    シンプルな三層ニューラルネットワーク分類器\n",
    "    Parameters\n",
    "    ----------\n",
    "    Attributes\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    def __init__(self, sigma, batch_size, n_features, n_nodes1, n_nodes2, n_nodes3, n_output, num_iter=10, lr=0.0005, verbose = False):\n",
    "        self.sigma = sigma # ガウス分布（正規分布）用の標準偏差\n",
    "        self.lr = lr # 学習率\n",
    "\n",
    "        self.batch_size = batch_size # バッチサイズ\n",
    "        self.n_features = n_features # 特徴量の数\n",
    "        self.n_nodes1 = n_nodes1 # 1層目のノード数\n",
    "        self.n_nodes2 = n_nodes2 # 2層目のノード数\n",
    "        self.n_nodes3 = n_nodes3 # 3層目のノード数\n",
    "        self.n_output = n_output # 出力のクラス数（3層目のノード数）\n",
    "        self.num_iter = num_iter # イテレーション回数\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, X, y, X_val=None, y_val=None, fp=\"sigmoid\"):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を学習する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "\n",
    "        print(\"特徴量ベクトル(batch_size, n_features)：{}\".format(X.shape))\n",
    "        print(\"バッチサイズ(self.batch_size) : {}\".format(self.batch_size)) # バッチサイズ\n",
    "        print(\"特徴量の数(self.n_features) : {}\".format(self.n_features)) # 特徴量の数\n",
    "        print(\"1層目のノード数(self.n_nodes1) : {}\".format(self.n_nodes1)) # 1層目のノード数\n",
    "        print(\"2層目のノード数(self.n_nodes2) : {}\".format(self.n_nodes2)) # 2層目のノード数\n",
    "        print(\"3層目のノード数(self.n_nodes3) : {}\".format(self.n_nodes3)) # 2層目のノード数\n",
    "        print(\"出力のクラス数（3層目のノード数）(self.n_output): {}\".format(self.n_output)) # 出力のクラス数（3層目のノード数）\n",
    "\n",
    "        # self.sigma : ガウス分布の標準偏差\n",
    "        # self.lr : 学習率\n",
    "        # self.n_nodes1 : 1層目のノード数\n",
    "        # self.n_nodes2 : 2層目のノード数\n",
    "        # self.n_output : 出力層のノード数\n",
    "\n",
    "        self.cee_array = []\n",
    "\n",
    "        for num_iter in range(self.num_iter):\n",
    "\n",
    "            # 初期化と宣言\n",
    "            optimizer1 = SGD(self.lr)\n",
    "            optimizer2 = SGD(self.lr)\n",
    "            optimizer3 = SGD(self.lr)\n",
    "            optimizer4 = SGD(self.lr)\n",
    "#             optimizer1 = AdaGrad(self.lr)\n",
    "#             optimizer2 = AdaGrad(self.lr)\n",
    "#             optimizer3 = AdaGrad(self.lr)\n",
    "#             optimizer4 = AdaGrad(self.lr)\n",
    "#             s_initializer_1 = SimpleInitializer(self.sigma)\n",
    "#             s_initializer_2 = SimpleInitializer(self.sigma)\n",
    "#             s_initializer_3 = SimpleInitializer(self.sigma)\n",
    "#             s_initializer_4 = SimpleInitializer(self.sigma)\n",
    "            h_initializer_1 = HeInitializer()\n",
    "            h_initializer_2 = HeInitializer()\n",
    "            h_initializer_3 = HeInitializer()\n",
    "            h_initializer_4 = HeInitializer()\n",
    "            self.FC1 = FC(self.n_features, self.n_nodes1, h_initializer_1, optimizer1)\n",
    "            self.activation1 = Tanh()\n",
    "#             self.activation1 = ReLU()\n",
    "            self.FC2 = FC(self.n_nodes1, self.n_nodes2, h_initializer_2, optimizer2)\n",
    "            self.activation2 = Tanh()\n",
    "#             self.activation2 = ReLU()\n",
    "            self.FC3 = FC(self.n_nodes2, self.n_nodes3, h_initializer_3, optimizer3)\n",
    "            self.activation3 = Tanh()\n",
    "#             self.activation3 = ReLU()\n",
    "            self.FC4 = FC(self.n_nodes3, self.n_output, h_initializer_4, optimizer4)\n",
    "            self.activation4 = Softmax()\n",
    "\n",
    "            # ミニバッチ処理\n",
    "            get_mini_batch = GetMiniBatch(X, y, self.batch_size)\n",
    "            # get_mini_batch = GetMiniBatch(X, y, batch_size=20)\n",
    "\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "\n",
    "                ### フォワードプロバゲーション\n",
    "\n",
    "                ### 1層目\n",
    "                A1 = self.FC1.forward(mini_X_train)\n",
    "                Z1 = self.activation1.forward(A1)\n",
    "\n",
    "                ### 2層目\n",
    "                A2 = self.FC2.forward(Z1)\n",
    "                Z2 = self.activation2.forward(A2)\n",
    "\n",
    "                ### 3層目\n",
    "                A3 = self.FC3.forward(Z2)\n",
    "                Z3 = self.activation3.forward(A3)\n",
    "\n",
    "                ### 4層目\n",
    "                A4 = self.FC4.forward(Z3)\n",
    "                Z4 = self.activation4.forward(A4)\n",
    "\n",
    "                ### バックプロバゲーション\n",
    "                ### 4層目\n",
    "                dA4 = self.activation4.backward(Z4, mini_y_train) # 交差エントロピー誤差とソフトマックスを合わせている\n",
    "                dZ3 = self.FC4.backward(dA4)\n",
    "\n",
    "                ### 3層目\n",
    "                dA3 = self.activation3.backward(dZ3)\n",
    "                dZ2 = self.FC3.backward(dA3)\n",
    "\n",
    "                ### 2層目\n",
    "                dA2 = self.activation2.backward(dZ2)\n",
    "                dZ1 = self.FC2.backward(dA2)\n",
    "\n",
    "                ### 1層目\n",
    "                dA1 = self.activation1.backward(dZ1)\n",
    "                dZ0 = self.FC1.backward(dA1) # dZ0は使用しない\n",
    "\n",
    "            # 交差エントロピーの記録\n",
    "            self.cee_array.append(self.activation4.L)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        ニューラルネットワーク分類器を使い推定する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            推定結果\n",
    "        \"\"\"\n",
    "        ### フォワードプロバゲーション\n",
    "        ### 1層目\n",
    "        A1 = self.FC1.forward(X)\n",
    "        Z1 = self.activation1.forward(A1)\n",
    "        ### 2層目\n",
    "        A2 = self.FC2.forward(Z1)\n",
    "        Z2 = self.activation2.forward(A2)\n",
    "        ### 3層目\n",
    "        A3 = self.FC3.forward(Z2)\n",
    "        Z3 = self.activation3.forward(A3)\n",
    "        ### 4層目\n",
    "        A4 = self.FC4.forward(Z3)\n",
    "        Z4 = self.activation4.forward(A4)\n",
    "\n",
    "        # 最も高い確率(z3内のデータ)を判定\n",
    "        max_Z4_index = np.argmax(Z4)\n",
    "        max_Z4 = Z4.flatten()[max_Z4_index]\n",
    "\n",
    "        return max_Z4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54f0ea29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "特徴量ベクトル(batch_size, n_features)：(48000, 784)\n",
      "バッチサイズ(self.batch_size) : 20\n",
      "特徴量の数(self.n_features) : 784\n",
      "1層目のノード数(self.n_nodes1) : 400\n",
      "2層目のノード数(self.n_nodes2) : 200\n",
      "3層目のノード数(self.n_nodes3) : 100\n",
      "出力のクラス数（3層目のノード数）(self.n_output): 10\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "0.001\n",
      "1.0000000000333444e-05\n"
     ]
    }
   ],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\n",
    "\n",
    "# クラス宣言時の引数\n",
    "sigma = 0.01 # ガウス分布（正規分布）用の標準偏差\n",
    "batch_size = 20 # バッチサイズ\n",
    "n_features = 784 # 特徴量の数\n",
    "n_nodes1 = 400 # 1層目のノード数\n",
    "n_nodes2 = 200 # 2層目のノード数\n",
    "n_nodes3 = 100 # 3層目のノード数\n",
    "n_output = 10 # 出力のクラス数（4層目のノード数）\n",
    "num_iter = 5 # イテレーション回数\n",
    "lr = 0.001 # 学習率\n",
    "verbose = False\n",
    "\n",
    "# インスタンス化\n",
    "sdnnc_n = ScratchDeepNeuralNetrowkClassifier_N(sigma, batch_size, n_features, n_nodes1, n_nodes2, n_nodes3, n_output, num_iter, lr, verbose)\n",
    "# fit関数呼び出し\n",
    "sdnnc_n.fit(X_train, y_train, X_val, y_val)\n",
    "accuracy = sdnnc_n.predict(X_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4a6f15e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5.298297366717712, 5.298297366771672, 5.298297366771672, 5.298297366771672, 5.298297366771672]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(len(sdnnc_n.cee_array)), sdnnc_n.cee_array)\n",
    "print(sdnnc_n.cee_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610aab8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題1】2次元畳み込み層の作成\r\n",
    "### 【問題3】2次元畳み込み後の出力サイズ\r\n",
    "### 【問題4】最大プーリング層の作成\r\n",
    "### 【問題7】学習と推定"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "source": [
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from collections import OrderedDict\r\n",
    "import tensorflow as tf\r\n",
    "# from keras.datasets import mnist\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "source": [
    "# Relu関数\r\n",
    "class Relu:\r\n",
    "    def __init__(self):\r\n",
    "        self.mask = None\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        self.mask = (x <= 0)\r\n",
    "        out = x.copy()\r\n",
    "        out[self.mask] = 0\r\n",
    "\r\n",
    "        return out\r\n",
    "\r\n",
    "    def backward(self, dout):\r\n",
    "        dout[self.mask] = 0\r\n",
    "        dx = dout\r\n",
    "\r\n",
    "        return dx\r\n",
    "\r\n",
    "# Sigmoid関数\r\n",
    "class Sigmoid:\r\n",
    "    def __init__(self):\r\n",
    "        self.out = None\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        out = sigmoid(x)\r\n",
    "        self.out = out\r\n",
    "        return out\r\n",
    "\r\n",
    "    def backward(self, dout):\r\n",
    "        dx = dout * (1.0 - self.out) * self.out\r\n",
    "\r\n",
    "        return dx\r\n",
    "\r\n",
    "# Affine関数\r\n",
    "class Affine:\r\n",
    "    def __init__(self, W, b):\r\n",
    "        self.W =W\r\n",
    "        self.b = b\r\n",
    "        \r\n",
    "        self.x = None\r\n",
    "        self.original_x_shape = None\r\n",
    "        # 重み・バイアスパラメータの微分\r\n",
    "        self.dW = None\r\n",
    "        self.db = None\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # テンソル対応\r\n",
    "        self.original_x_shape = x.shape\r\n",
    "        x = x.reshape(x.shape[0], -1)\r\n",
    "        self.x = x\r\n",
    "\r\n",
    "        out = np.dot(self.x, self.W) + self.b\r\n",
    "\r\n",
    "        return out\r\n",
    "\r\n",
    "    def backward(self, dout):\r\n",
    "        dx = np.dot(dout, self.W.T)\r\n",
    "        self.dW = np.dot(self.x.T, dout)\r\n",
    "        self.db = np.sum(dout, axis=0)\r\n",
    "        \r\n",
    "        dx = dx.reshape(*self.original_x_shape)  # 入力データの形状に戻す（テンソル対応）\r\n",
    "        return dx\r\n",
    "\r\n",
    "# Softmax関数\r\n",
    "def softmax(x):\r\n",
    "    x = x - np.max(x, axis=-1, keepdims=True)   # オーバーフロー対策\r\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\r\n",
    "\r\n",
    "# 交差エントロピー誤差\r\n",
    "def cross_entropy_error(y, t):\r\n",
    "    if y.ndim == 1:\r\n",
    "        t = t.reshape(1, t.size)\r\n",
    "        y = y.reshape(1, y.size)\r\n",
    "        \r\n",
    "    # 教師データがone-hot-vectorの場合、正解ラベルのインデックスに変換\r\n",
    "    if t.size == y.size:\r\n",
    "        t = t.argmax(axis=1)\r\n",
    "             \r\n",
    "    batch_size = y.shape[0]\r\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\r\n",
    "\r\n",
    "# Softmax関数とロス関数\r\n",
    "class SoftmaxWithLoss:\r\n",
    "    def __init__(self):\r\n",
    "        self.loss = None\r\n",
    "        self.y = None # softmaxの出力\r\n",
    "        self.t = None # 教師データ\r\n",
    "\r\n",
    "    def forward(self, x, t):\r\n",
    "        self.t = t\r\n",
    "        self.y = softmax(x)\r\n",
    "        self.loss = cross_entropy_error(self.y, self.t)\r\n",
    "        \r\n",
    "        return self.loss\r\n",
    "\r\n",
    "    def backward(self, dout=1):\r\n",
    "        batch_size = self.t.shape[0]\r\n",
    "        if self.t.size == self.y.size: # 教師データがone-hot-vectorの場合\r\n",
    "            dx = (self.y - self.t) / batch_size\r\n",
    "        else:\r\n",
    "            dx = self.y.copy()\r\n",
    "            dx[np.arange(batch_size), self.t] -= 1\r\n",
    "            dx = dx / batch_size\r\n",
    "        \r\n",
    "        return dx\r\n",
    "\r\n",
    "# 最適化\r\n",
    "class SGD:\r\n",
    "\r\n",
    "    \"\"\"確率的勾配降下法（Stochastic Gradient Descent）\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, lr=0.01):\r\n",
    "        self.lr = lr\r\n",
    "        \r\n",
    "    def update(self, params, grads):\r\n",
    "        for key in params.keys():\r\n",
    "            params[key] -= self.lr * grads[key] \r\n",
    "\r\n",
    "# 最適化\r\n",
    "class Adam:\r\n",
    "\r\n",
    "    \"\"\"Adam (http://arxiv.org/abs/1412.6980v8)\"\"\"\r\n",
    "\r\n",
    "    def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):\r\n",
    "        self.lr = lr\r\n",
    "        self.beta1 = beta1\r\n",
    "        self.beta2 = beta2\r\n",
    "        self.iter = 0\r\n",
    "        self.m = None\r\n",
    "        self.v = None\r\n",
    "        \r\n",
    "    def update(self, params, grads):\r\n",
    "        if self.m is None:\r\n",
    "            self.m, self.v = {}, {}\r\n",
    "            for key, val in params.items():\r\n",
    "                self.m[key] = np.zeros_like(val)\r\n",
    "                self.v[key] = np.zeros_like(val)\r\n",
    "        \r\n",
    "        self.iter += 1\r\n",
    "        lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         \r\n",
    "        \r\n",
    "        for key in params.keys():\r\n",
    "            #self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]\r\n",
    "            #self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)\r\n",
    "            self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])\r\n",
    "            self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])\r\n",
    "            \r\n",
    "            params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)\r\n",
    "            \r\n",
    "            #unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias\r\n",
    "            #unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias\r\n",
    "            #params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "source": [
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\r\n",
    "    \"\"\"\r\n",
    "    関数の概要：入力データを計算しやすいように二次元へ変換する\r\n",
    "               ※ブラックボックス扱いでよい\r\n",
    "    ----------\r\n",
    "    Parameters\r\n",
    "        input_data : (データ数, チャンネル, 高さ, 幅)の4次元配列からなる入力データ\r\n",
    "        filter_h : フィルターの高さ\r\n",
    "        filter_w : フィルターの幅\r\n",
    "        stride : ストライド\r\n",
    "        pad : パディング\r\n",
    "    ----------\r\n",
    "    Returns\r\n",
    "        col : 2次元配列\r\n",
    "    \"\"\"\r\n",
    "    N, C, H, W = input_data.shape\r\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\r\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\r\n",
    "\r\n",
    "    img = np.pad(input_data, [(0,0), (0,0), (pad, pad), (pad, pad)], 'constant')\r\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\r\n",
    "\r\n",
    "    for y in range(filter_h):\r\n",
    "        y_max = y + stride*out_h\r\n",
    "        for x in range(filter_w):\r\n",
    "            x_max = x + stride*out_w\r\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\r\n",
    "\r\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N*out_h*out_w, -1)\r\n",
    "    return col\r\n",
    "\r\n",
    "def col2im(col, input_shape, filter_h, filter_w, stride=1, pad=0):\r\n",
    "    \"\"\"\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    col :\r\n",
    "    input_shape : 入力データの形状（例：(10, 1, 28, 28)）\r\n",
    "    filter_h :\r\n",
    "    filter_w\r\n",
    "    stride\r\n",
    "    pad\r\n",
    "    Returns\r\n",
    "    -------\r\n",
    "    \"\"\"\r\n",
    "    N, C, H, W = input_shape\r\n",
    "    out_h = (H + 2*pad - filter_h)//stride + 1\r\n",
    "    out_w = (W + 2*pad - filter_w)//stride + 1\r\n",
    "    col = col.reshape(N, out_h, out_w, C, filter_h, filter_w).transpose(0, 3, 4, 5, 1, 2)\r\n",
    "\r\n",
    "    img = np.zeros((N, C, H + 2*pad + stride - 1, W + 2*pad + stride - 1))\r\n",
    "    for y in range(filter_h):\r\n",
    "        y_max = y + stride*out_h\r\n",
    "        for x in range(filter_w):\r\n",
    "            x_max = x + stride*out_w\r\n",
    "            img[:, :, y:y_max:stride, x:x_max:stride] += col[:, :, y, x, :, :]\r\n",
    "\r\n",
    "    return img[:, :, pad:H + pad, pad:W + pad]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "# 畳み込み層(Convolutionレイヤ)\r\n",
    "class Convolution:\r\n",
    "    def __init__(self, W, b, stride=1, pad=0):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要 ： 畳み込み層用のコンストラクタ\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            W : 重み\r\n",
    "            b : バイアス\r\n",
    "            stride : ストライド\r\n",
    "            pad : パディングする幅(1なら周囲に1つパディング)\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            －\r\n",
    "        \"\"\"\r\n",
    "        self.W = W\r\n",
    "        self.b = b\r\n",
    "        self.stride = stride\r\n",
    "        self.pad = pad\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要 ： 畳み込み層のフォワードプロパゲーション\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            x : 入力データ\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            out : 出力データ\r\n",
    "        \"\"\"\r\n",
    "        # 重み（フィルター）の各次元要素数をそれぞれの変数へ代入\r\n",
    "        # 　FN : フィルター数（ハイパーパラメータ）\r\n",
    "        # 　C  : チャンネル数（入力データ(x)と同じ値である必要がある）\r\n",
    "        # 　FH : フィルターの縦幅\r\n",
    "        # 　FW : フィルターの横幅\r\n",
    "        FN, C, FH, FW = self.W.shape\r\n",
    "        # 入力データの各次元要素数をそれぞれの変数へ代入\r\n",
    "        # 　N : バッチ数\r\n",
    "        # 　C : チャンネル数（重み(フィルター)(self.W)と同じ値である必要がある）\r\n",
    "        # 　H : 入力データの縦幅\r\n",
    "        # 　W : 入力データの横幅\r\n",
    "        N, C, H, W = x.shape\r\n",
    "\r\n",
    "        # 出力データのサイズを計算\r\n",
    "        out_h = 1 + int((H + 2*self.pad - FH) / self.stride)\r\n",
    "        out_w = 1 + int((W + 2*self.pad - FW) / self.stride)\r\n",
    "        # out_h = 1 + (H + 2*self.pad - FN) // self.stride\r\n",
    "        # out_w = 1 + (W + 2*self.pad - FW) // self.stride\r\n",
    "\r\n",
    "        # 出力データを計算しやすくするために、im2col関数を通してデータを2次元にする\r\n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\r\n",
    "        # im2colの出力結果と計算するために、重みを変形\r\n",
    "        col_W = self.W.reshape(FN, -1).T\r\n",
    "\r\n",
    "        # 出力データの計算\r\n",
    "        out = col @ col_W + self.b\r\n",
    "        # 出力データをreshapeした後、次元を戻す(N, H, W, C) ⇒ (N, C, H, W)\r\n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\r\n",
    "\r\n",
    "        # backward用に値を保存\r\n",
    "        self.x = x\r\n",
    "        self.col = col\r\n",
    "        self.col_W = col_W\r\n",
    "\r\n",
    "        # 出力データを返す\r\n",
    "        return out\r\n",
    "\r\n",
    "    def backward(self, dout):\r\n",
    "        FN, C, FH, FW = self.W.shape\r\n",
    "        dout = dout.transpose(0,2,3,1).reshape(-1, FN)\r\n",
    "\r\n",
    "        self.db = np.sum(dout, axis=0)\r\n",
    "        self.dW = np.dot(self.col.T, dout)\r\n",
    "        self.dW = self.dW.transpose(1, 0).reshape(FN, C, FH, FW)\r\n",
    "\r\n",
    "        dcol = np.dot(dout, self.col_W.T)\r\n",
    "        dx = col2im(dcol, self.x.shape, FH, FW, self.stride, self.pad)\r\n",
    "\r\n",
    "        return dx\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "source": [
    "# Poolingクラスの定義\r\n",
    "class Pooling:\r\n",
    "    def __init__(self, pool_h, pool_w, stride=2, pad=0):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要 ： プーリング層用のコンストラクタ\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            pool_h : プーリングするフィルタの高さ\r\n",
    "            pool_w : プーリングするフィルタの幅\r\n",
    "            stride : ストライド\r\n",
    "            pad : パディングする幅(1なら周囲に1つパディング)\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            －\r\n",
    "        \"\"\"\r\n",
    "        # 初期化\r\n",
    "        self.pool_h = pool_h\r\n",
    "        self.pool_w = pool_w\r\n",
    "        self.stride = stride\r\n",
    "        self.pad = pad\r\n",
    "        # バックプロパゲーション用の変数\r\n",
    "        self.x = None\r\n",
    "        self.arg_max = None\r\n",
    "    \r\n",
    "    def forward(self, x):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要 ： プーリング層のフォワードプロパゲーション\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            x : 入力データ\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            out : 出力データ\r\n",
    "        \"\"\"\r\n",
    "        # 入力データの各次元要素数をそれぞれの変数へ代入\r\n",
    "        # 　N : データ数\r\n",
    "        # 　C : チャンネル数\r\n",
    "        # 　H : 入力データの縦幅\r\n",
    "        # 　W : 入力データの横幅\r\n",
    "        N, C, H, W = x.shape\r\n",
    "\r\n",
    "        # 出力データのサイズを計算\r\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\r\n",
    "        out_w = int(1 + (W - self.pool_w) / self.stride)\r\n",
    "\r\n",
    "        # im2col関数を使用して、2次元データへ変換\r\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\r\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\r\n",
    "\r\n",
    "        # 2次元にしたデータから最大値を取得\r\n",
    "        out = np.max(col, axis=1)\r\n",
    "        # 出力データの形へ戻す\r\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\r\n",
    "\r\n",
    "        # バックプロパゲーション用に入力データとプーリングした最大値のインデックスを保存しておく\r\n",
    "        self.x = x\r\n",
    "        self.arg_max = np.argmax(col, axis=1)\r\n",
    "\r\n",
    "        return out\r\n",
    "\r\n",
    "    def backward(self, dout):\r\n",
    "        dout = dout.transpose(0, 2, 3, 1)\r\n",
    "        \r\n",
    "        pool_size = self.pool_h * self.pool_w\r\n",
    "        dmax = np.zeros((dout.size, pool_size))\r\n",
    "        dmax[np.arange(self.arg_max.size), self.arg_max.flatten()] = dout.flatten()\r\n",
    "        dmax = dmax.reshape(dout.shape + (pool_size,)) \r\n",
    "        \r\n",
    "        dcol = dmax.reshape(dmax.shape[0] * dmax.shape[1] * dmax.shape[2], -1)\r\n",
    "        dx = col2im(dcol, self.x.shape, self.pool_h, self.pool_w, self.stride, self.pad)\r\n",
    "        \r\n",
    "        return dx\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "source": [
    "# Scratch2dCNNClassifierクラスの定義\r\n",
    "class Scratch2dCNNClassifier:\r\n",
    "    \"\"\"\r\n",
    "    クラスの概要：次の構造をしている畳み込みニューラルネットワーク\r\n",
    "    　conv - relu - pool - affine - relu - affine - softmax\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self,\r\n",
    "                 input_dim=(1, 28, 28),\r\n",
    "                 conv_param={\"filter_num\":30,\r\n",
    "                             \"filter_size\":5,\r\n",
    "                             \"stride\":1,\r\n",
    "                             \"pad\":0},\r\n",
    "                 hidden_size=100,\r\n",
    "                 output_size=10,\r\n",
    "                 weight_init_std=0.01):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要：SimpleConvNetのコンストラクタ\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            input_dim : 入力データの構造\r\n",
    "            conv_param : フィルターの構造を持った辞書型変数\r\n",
    "                filter_num : フィルターの数\r\n",
    "                filter_size : チャンネル数\r\n",
    "                stride : ストライド\r\n",
    "                pad : パディング\r\n",
    "            hidden_size : 隠れ層のニューロン数（ハイパーパラメータ）\r\n",
    "            output_size : 最終出力結果となる要素数（ハイパーパラメータ）\r\n",
    "            weight_init_std : 重みの標準偏差\r\n",
    "                \"relu\"または\"he\"の場合は「heの初期値」を設定\r\n",
    "                \"sigmoid\"または\"xavier\"の場合は「xavierの初期値」を設定\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            －\r\n",
    "        \"\"\"\r\n",
    "        # 初期値の設定\r\n",
    "        filter_num = conv_param[\"filter_num\"]\r\n",
    "        filter_size = conv_param[\"filter_size\"]\r\n",
    "        filter_stride = conv_param[\"stride\"]\r\n",
    "        filter_pad = conv_param[\"pad\"]\r\n",
    "        input_size = input_dim[1]   # 入力データの行数\r\n",
    "\r\n",
    "        # 折り畳み層の出力データのサイズ\r\n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) // filter_stride + 1\r\n",
    "        # プール層の出力データのサイズ\r\n",
    "        pool_output_size = int(filter_num * (conv_output_size / 2) * (conv_output_size / 2))\r\n",
    "\r\n",
    "        # 重みとバイアスの初期化\r\n",
    "        self.params = {}\r\n",
    "        self.params[\"W1\"] = weight_init_std * np.random.rand(filter_num, input_dim[0], filter_size, filter_size)\r\n",
    "        self.params[\"b1\"] = np.zeros(filter_num)\r\n",
    "        self.params[\"W2\"] = weight_init_std * np.random.rand(pool_output_size, hidden_size)\r\n",
    "        self.params[\"b2\"] = np.zeros(hidden_size)\r\n",
    "        self.params[\"W3\"] = weight_init_std * np.random.rand(hidden_size, output_size)\r\n",
    "        self.params[\"b3\"] = np.zeros(output_size)\r\n",
    "\r\n",
    "        # レイヤの生成\r\n",
    "        self.layers = OrderedDict()\r\n",
    "        self.layers[\"Conv1\"] = Convolution(self.params[\"W1\"],\r\n",
    "                                           self.params[\"b1\"],\r\n",
    "                                           conv_param[\"stride\"],\r\n",
    "                                           conv_param[\"pad\"])\r\n",
    "        self.layers[\"Relu1\"] = Relu()\r\n",
    "        self.layers[\"Pool1\"] = Pooling(pool_h=2, pool_w=2, stride=2)\r\n",
    "        self.layers[\"Affine1\"] = Affine(self.params[\"W2\"], self.params[\"b2\"])\r\n",
    "        self.layers[\"Relu2\"] = Relu()\r\n",
    "        self.layers[\"Affine2\"] = Affine(self.params[\"W3\"], self.params[\"b3\"])\r\n",
    "        # 推定や損失を求めるために他のレイヤと構造を分ける(他レイヤと同じ変数にしない)\r\n",
    "        self.last_layer = SoftmaxWithLoss()\r\n",
    "\r\n",
    "    # 推定\r\n",
    "    def predict(self, x):\r\n",
    "        for layer in self.layers.values():\r\n",
    "            x = layer.forward(x)\r\n",
    "\r\n",
    "        return x\r\n",
    "\r\n",
    "    # 損失\r\n",
    "    def loss(self, x, t):\r\n",
    "        y = self.predict(x)\r\n",
    "        return self.last_layer.forward(y, t)\r\n",
    "\r\n",
    "    # 正確性\r\n",
    "    def accuracy(self, x, t, batch_size=100):\r\n",
    "        if t.ndim != 1 : t = np.argmax(t, axis=1)\r\n",
    "        acc = 0.0\r\n",
    "        for i in range(int(x.shape[0] / batch_size)):\r\n",
    "            tx = x[i*batch_size:(i+1)*batch_size]\r\n",
    "            tt = t[i*batch_size:(i+1)*batch_size]\r\n",
    "            y = self.predict(tx)\r\n",
    "            y = np.argmax(y, axis=1)\r\n",
    "            acc += np.sum(y == tt) \r\n",
    "        \r\n",
    "        return acc / x.shape[0]\r\n",
    "\r\n",
    "    # 勾配\r\n",
    "    def gradient(self, x, t):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要：勾配を求める（誤差逆伝搬法）\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            x : 入力データ\r\n",
    "            t : 教師ラベル\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            各層の勾配を持ったディクショナリ変数\r\n",
    "                grads['W1']、grads['W2']、...は各層の重み\r\n",
    "                grads['b1']、grads['b2']、...は各層のバイアス\r\n",
    "        \"\"\"\r\n",
    "        # forward\r\n",
    "        self.loss(x, t)\r\n",
    "\r\n",
    "        # backward\r\n",
    "        dout = 1\r\n",
    "        dout = self.last_layer.backward(dout)\r\n",
    "\r\n",
    "        layers = list(self.layers.values())\r\n",
    "        layers.reverse()\r\n",
    "        for layer in layers:\r\n",
    "            dout = layer.backward(dout)\r\n",
    "\r\n",
    "        # 設定\r\n",
    "        grads = {}\r\n",
    "        grads['W1'], grads['b1'] = self.layers['Conv1'].dW, self.layers['Conv1'].db\r\n",
    "        grads['W2'], grads['b2'] = self.layers['Affine1'].dW, self.layers['Affine1'].db\r\n",
    "        grads['W3'], grads['b3'] = self.layers['Affine2'].dW, self.layers['Affine2'].db\r\n",
    "\r\n",
    "        return grads\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# Trainerクラスの定義\r\n",
    "class Trainer:\r\n",
    "    \"\"\"\r\n",
    "    クラスの概要：学習を行うクラス\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, network, x_train, t_train, x_test, t_test,\r\n",
    "                 epochs=20,\r\n",
    "                 mini_batch_size=100,\r\n",
    "                 optimizer='SGD',\r\n",
    "                 optimizer_param={'lr':0.01}, \r\n",
    "                 evaluate_sample_num_per_epoch=None,\r\n",
    "                 verbose=True):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要：Trainerのコンストラクタ\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            network : 学習に使用するNN(今回はSimpleConvNet)\r\n",
    "            x_train : 訓練データ\r\n",
    "            t_train : 訓練データの教師データ\r\n",
    "            x_train : 検証データ\r\n",
    "            t_train : 検証データの教師データ\r\n",
    "            epochs : 学習回数\r\n",
    "            mini_batch_size : バッチサイズ\r\n",
    "            optimizer : 最適化手法\r\n",
    "            optimizer_param : 最適化手法で使用するパラメータ\r\n",
    "                lr : 学習率\r\n",
    "            evaluate_sample_num_per_epoch : 1学習で使用するサンプル数\r\n",
    "            verbose : データの表示有無\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            －\r\n",
    "        \"\"\"\r\n",
    "        # 初期化\r\n",
    "        self.network = network\r\n",
    "        self.x_train = x_train\r\n",
    "        self.t_train = t_train\r\n",
    "        self.x_test = x_test\r\n",
    "        self.t_test = t_test\r\n",
    "        self.epochs = epochs\r\n",
    "        self.batch_size = mini_batch_size\r\n",
    "        self.evaluate_sample_num_per_epoch = evaluate_sample_num_per_epoch\r\n",
    "        self.verbose = verbose\r\n",
    "\r\n",
    "        # optimizerの設定\r\n",
    "        # optimizer_class_dict = {'sgd':SGD, 'momentum':Momentum, 'nesterov':Nesterov,\r\n",
    "        #                         'adagrad':AdaGrad, 'rmsprop':RMSprop, 'adam':Adam}\r\n",
    "        optimizer_class_dict = {\"sgd\":SGD, \"adam\":Adam}\r\n",
    "        self.optimizer = optimizer_class_dict[optimizer.lower()](**optimizer_param)\r\n",
    "\r\n",
    "        self.train_size = x_train.shape[0]                              # 訓練データの行数\r\n",
    "        self.iter_per_epoch = max(self.train_size / mini_batch_size, 1) # 1学習時のループ回数\r\n",
    "        self.max_iter = int(epochs * self.iter_per_epoch)               # 学習回数\r\n",
    "        self.current_iter = 0       # 1学習内の現在のループ数\r\n",
    "        self.current_epoch = 0      # 現在の学習回数\r\n",
    "        self.train_loss_list = []   # 訓練データ学習時のロス関数の返り値\r\n",
    "        self.train_acc_list = []    # 訓練データの正確性\r\n",
    "        self.test_acc_list = []     # 検証データの正確性\r\n",
    "\r\n",
    "    def train_step(self):\r\n",
    "        \"\"\"\r\n",
    "        関数の概要：学習\r\n",
    "        ----------\r\n",
    "        Parameters\r\n",
    "            －\r\n",
    "        ----------\r\n",
    "        Returns\r\n",
    "            －\r\n",
    "        \"\"\"\r\n",
    "        # 初期化\r\n",
    "        batch_mask = np.random.choice(self.train_size, self.batch_size) # ミニバッチのサイズを設定\r\n",
    "        x_batch = self.x_train[batch_mask]  # 訓練データのミニバッチ\r\n",
    "        t_batch = self.t_train[batch_mask]  # 検証データのミニバッチ\r\n",
    "        # 初期化時点での勾配を求める\r\n",
    "        grads = self.network.gradient(x_batch, t_batch)\r\n",
    "        # 勾配を基に最適化\r\n",
    "        self.optimizer.update(self.network.params, grads)\r\n",
    "        # 最適化した状態での損失を求める\r\n",
    "        loss = self.network.loss(x_batch, t_batch)\r\n",
    "        self.train_loss_list.append(loss)   # 配列に損失を格納\r\n",
    "        # verbose==Trueであれば表示\r\n",
    "        if self.verbose: print(\"train loss:\" + str(loss))\r\n",
    "        # 1学習でのループ\r\n",
    "        if self.current_iter % self.iter_per_epoch == 0:\r\n",
    "            self.current_epoch += 1 # ループ回数をカウント +1\r\n",
    "            x_train_sample, t_train_sample = self.x_train, self.t_train # 代入\r\n",
    "            x_test_sample, t_test_sample = self.x_test, self.t_test     # 代入\r\n",
    "            # 1学習で使用するサンプル数（ハイパーパラメータ）\r\n",
    "            if not self.evaluate_sample_num_per_epoch is None:\r\n",
    "                t = self.evaluate_sample_num_per_epoch\r\n",
    "                # evaluate_sample_num_per_epochの値に合わせてデータサイズを調整\r\n",
    "                x_train_sample, t_train_sample = self.x_train[:t], self.t_train[:t]\r\n",
    "                x_test_sample, t_test_sample = self.x_test[:t], self.t_test[:t]\r\n",
    "            # 訓練データの正確性\r\n",
    "            train_acc = self.network.accuracy(x_train_sample, t_train_sample)\r\n",
    "            # 検証データの正確性\r\n",
    "            test_acc = self.network.accuracy(x_test_sample, t_test_sample)\r\n",
    "            self.train_acc_list.append(train_acc)\r\n",
    "            self.test_acc_list.append(test_acc)\r\n",
    "            # verbose==Trueであれば表示\r\n",
    "            if self.verbose: print(\"=== epoch:\" + str(self.current_epoch) + \", train acc:\" + str(train_acc) + \", test acc:\" + str(test_acc) + \" ===\")\r\n",
    "        self.current_iter += 1\r\n",
    "\r\n",
    "    def train(self):\r\n",
    "        # 学習回数分ループ\r\n",
    "        for i in range(self.max_iter):\r\n",
    "            # 1回学習\r\n",
    "            self.train_step()\r\n",
    "        # (今回は)SimpleConvNetのaccracyクラスで正確性を測る\r\n",
    "        test_acc = self.network.accuracy(self.x_test, self.t_test)\r\n",
    "        # verbose==Trueなら検証データの正確性を表示\r\n",
    "        if self.verbose:\r\n",
    "            print(\"=============== Final Test Accuracy ===============\")\r\n",
    "            print(\"test acc:\" + str(test_acc))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "# MNISTデータの読み込み\r\n",
    "(x_train, t_train), (x_test, t_test) = tf.keras.datasets.mnist.load_data()\r\n",
    "\r\n",
    "# 処理に時間がかかるかもしれないので、データ数を削減(10分の1) \r\n",
    "x_train, t_train = x_train[:6000], t_train[:6000]\r\n",
    "x_test, t_test = x_test[:1000], t_test[:1000]\r\n",
    "\r\n",
    "# データ数、チャンネル数、縦、横、に変形\r\n",
    "x_train = x_train[:,np.newaxis,:,:]\r\n",
    "x_test = x_test[:,np.newaxis,:,:]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "source": [
    "# ハイパーパラメータの設定\r\n",
    "epochs = 2\r\n",
    "# epochs = 10\r\n",
    "\r\n",
    "# 学習に使用するScratch2dCNNClassifierのインスタンス化\r\n",
    "network = Scratch2dCNNClassifier(input_dim=(1,28,28), \r\n",
    "                        conv_param = {\"filter_num\": 30,\r\n",
    "                                      \"filter_size\": 5,\r\n",
    "                                      \"stride\": 1,\r\n",
    "                                      \"pad\": 0},\r\n",
    "                        hidden_size=100,\r\n",
    "                        output_size=10,\r\n",
    "                        weight_init_std=0.01)\r\n",
    "\r\n",
    "# 学習を行うTrinerのインスタンス化\r\n",
    "trainer = Trainer(network,                              # ニューラルネットワーク\r\n",
    "                  x_train,                              # 訓練データ\r\n",
    "                  t_train,                              # 訓練データの教師データ\r\n",
    "                  x_test,                               # 検証データ\r\n",
    "                  t_test,                               # 検証データの教師データ\r\n",
    "                  epochs=epochs,                        # 学習回数\r\n",
    "                  mini_batch_size=100,                  # ミニバッチサイズ\r\n",
    "                  optimizer='Adam',                     # 最適化手法\r\n",
    "                  optimizer_param={'lr': 0.001},        # 学習率\r\n",
    "                  evaluate_sample_num_per_epoch=1000)   # 1学習で使用するサンプル数\r\n",
    "\r\n",
    "# 学習\r\n",
    "trainer.train()\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train loss:5.765162547205821\n",
      "=== epoch:1, train acc:0.1, test acc:0.094 ===\n",
      "train loss:6.62129061778066\n",
      "train loss:6.518560465134152\n",
      "train loss:5.34377341436276\n",
      "train loss:3.374696259568249\n",
      "train loss:2.4172800602351567\n",
      "train loss:2.304260224726391\n",
      "train loss:2.2991431731892353\n",
      "train loss:2.303502081026874\n",
      "train loss:2.3023410384770537\n",
      "train loss:2.3028397667653913\n",
      "train loss:2.3025401055377643\n",
      "train loss:2.30267862111266\n",
      "train loss:2.3025741008145744\n",
      "train loss:2.302462336761758\n",
      "train loss:2.302182311236274\n",
      "train loss:2.3022833951767607\n",
      "train loss:2.3025640232008255\n",
      "train loss:2.302603211831326\n",
      "train loss:2.3022558983227746\n",
      "train loss:2.3019311674642413\n",
      "train loss:2.3020226025504704\n",
      "train loss:2.3026191776476304\n",
      "train loss:2.302840572376619\n",
      "train loss:2.302576873458815\n",
      "train loss:2.303089242908891\n",
      "train loss:2.302435017240725\n",
      "train loss:2.3024038131070736\n",
      "train loss:2.3018672911271394\n",
      "train loss:2.3023443449193053\n",
      "train loss:2.3023747517210955\n",
      "train loss:2.3024277978886527\n",
      "train loss:2.3024783136544857\n",
      "train loss:2.302571759362753\n",
      "train loss:2.3030741762663256\n",
      "train loss:2.3021650267377636\n",
      "train loss:2.3017939935968923\n",
      "train loss:2.301911855916248\n",
      "train loss:2.3028485749538232\n",
      "train loss:2.3024342024028206\n",
      "train loss:2.3029644948621284\n",
      "train loss:2.301977578012863\n",
      "train loss:2.3025192283810196\n",
      "train loss:2.3016523302722165\n",
      "train loss:2.302227939333713\n",
      "train loss:2.302510865566684\n",
      "train loss:2.3018397742036467\n",
      "train loss:2.30248861532756\n",
      "train loss:2.3027420252950175\n",
      "train loss:2.301290899610797\n",
      "train loss:2.302720833916489\n",
      "train loss:2.3023135999175786\n",
      "train loss:2.3014429850845817\n",
      "train loss:2.3013914378879226\n",
      "train loss:2.3027798704573543\n",
      "train loss:2.3021364245434763\n",
      "train loss:2.3029294931608537\n",
      "train loss:2.3020575906728733\n",
      "train loss:2.3023258578029804\n",
      "train loss:2.302507054611338\n",
      "train loss:2.304539817864482\n",
      "=== epoch:2, train acc:0.117, test acc:0.099 ===\n",
      "train loss:2.3013882348390275\n",
      "train loss:2.3015942500396296\n",
      "train loss:2.302517159914248\n",
      "train loss:2.3027667621193584\n",
      "train loss:2.302117035697578\n",
      "train loss:2.3024206003061782\n",
      "train loss:2.302788213436352\n",
      "train loss:2.303916634211771\n",
      "train loss:2.302319759937689\n",
      "train loss:2.300751513107568\n",
      "train loss:2.302388955666113\n",
      "train loss:2.302314241649191\n",
      "train loss:2.301046108214606\n",
      "train loss:2.3025191127915576\n",
      "train loss:2.3016103577481015\n",
      "train loss:2.3002812568537965\n",
      "train loss:2.301453891749719\n",
      "train loss:2.303096616502078\n",
      "train loss:2.2991950271520225\n",
      "train loss:2.302360044519392\n",
      "train loss:2.3028539378146493\n",
      "train loss:2.3037971002656916\n",
      "train loss:2.302762566595313\n",
      "train loss:2.302170539947441\n",
      "train loss:2.302740850221002\n",
      "train loss:2.302103815191453\n",
      "train loss:2.3042398103820214\n",
      "train loss:2.3005819996999524\n",
      "train loss:2.3017770111612066\n",
      "train loss:2.300797650667072\n",
      "train loss:2.301644906107194\n",
      "train loss:2.3031256701748988\n",
      "train loss:2.302256300968832\n",
      "train loss:2.3004526866138875\n",
      "train loss:2.2995374126409165\n",
      "train loss:2.3025021347116517\n",
      "train loss:2.301817003503862\n",
      "train loss:2.3028700458668006\n",
      "train loss:2.3008742510895406\n",
      "train loss:2.298760781707443\n",
      "train loss:2.3023547903551322\n",
      "train loss:2.301543473320802\n",
      "train loss:2.3018254474581443\n",
      "train loss:2.303114131838404\n",
      "train loss:2.2991241044690285\n",
      "train loss:2.3040170602890524\n",
      "train loss:2.3029580445845013\n",
      "train loss:2.3026496672809826\n",
      "train loss:2.3007804253333908\n",
      "train loss:2.3021602839346538\n",
      "train loss:2.3010251461675786\n",
      "train loss:2.3002952098487324\n",
      "train loss:2.3029732792524302\n",
      "train loss:2.2990887588847766\n",
      "train loss:2.302509813019748\n",
      "train loss:2.302117328842887\n",
      "train loss:2.300768352091205\n",
      "train loss:2.3019731079403796\n",
      "train loss:2.3023338920179883\n",
      "=============== Final Test Accuracy ===============\n",
      "test acc:0.099\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "source": [
    "# グラフの描画\r\n",
    "markers = {'train': 'o', 'test': 's'}\r\n",
    "x = np.arange(epochs)\r\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\r\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\r\n",
    "plt.xlabel(\"epochs\")\r\n",
    "plt.ylabel(\"accuracy\")\r\n",
    "plt.ylim(0, 1.0)\r\n",
    "plt.legend(loc='lower right')\r\n",
    "plt.show()\r\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DIC01env': conda)"
  },
  "interpreter": {
   "hash": "6b7f8bdceed5477f092ac7c41d593afd8321a641dc4c59fe20ae9add7ebabcd8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
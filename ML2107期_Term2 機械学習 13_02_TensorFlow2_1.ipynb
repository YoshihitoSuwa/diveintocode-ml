{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題1】スクラッチを振り返る\r\n",
    "ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\r\n",
    "　・入力層、畳み込み層、プーリング層、全結合層、出力層から成る\r\n",
    "　・入力層⇒（畳み込み層⇒プーリング層）×ｎ回繰り返し⇒畳み込み層⇒全結合層⇒出力層、の流れ\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題2】スクラッチとTensorFlowの対応を考える\r\n",
    "以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\r\n",
    "それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "\"\"\"\r\n",
    "TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\r\n",
    "\"\"\"\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import tensorflow as tf\r\n",
    "# データセットの読み込み\r\n",
    "dataset_path =\"Iris.csv\"\r\n",
    "df = pd.read_csv(dataset_path)\r\n",
    "# データフレームから条件抽出\r\n",
    "df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\r\n",
    "y = df[\"Species\"]\r\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\r\n",
    "y = np.array(y)\r\n",
    "X = np.array(X).astype(np.float32)\r\n",
    "# ラベルを数値に変換\r\n",
    "y[y=='Iris-versicolor'] = 0\r\n",
    "y[y=='Iris-virginica'] = 1\r\n",
    "y = y.astype(np.float32)[:, np.newaxis]\r\n",
    "# trainとtestに分割\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
    "# さらにtrainとvalに分割\r\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\r\n",
    "class GetMiniBatch:\r\n",
    "    \"\"\"\r\n",
    "    ミニバッチを取得するイテレータ\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\r\n",
    "      訓練データ\r\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\r\n",
    "      正解値\r\n",
    "    batch_size : int\r\n",
    "      バッチサイズ\r\n",
    "    seed : int\r\n",
    "      NumPyの乱数のシード\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\r\n",
    "        self.batch_size = batch_size\r\n",
    "        np.random.seed(seed)\r\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
    "        self.X = X[shuffle_index]\r\n",
    "        self.y = y[shuffle_index]\r\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\r\n",
    "        # self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
    "    def __len__(self):\r\n",
    "        return self._stop\r\n",
    "    def __getitem__(self,item):\r\n",
    "        p0 = item*self.batch_size\r\n",
    "        p1 = item*self.batch_size + self.batch_size\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \r\n",
    "    def __iter__(self):\r\n",
    "        self._counter = 0\r\n",
    "        return self\r\n",
    "    def __next__(self):\r\n",
    "        if self._counter >= self._stop:\r\n",
    "            raise StopIteration()\r\n",
    "        p0 = self._counter*self.batch_size\r\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
    "        self._counter += 1\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]\r\n",
    "# ハイパーパラメータの設定\r\n",
    "learning_rate = 0.01\r\n",
    "batch_size = 10\r\n",
    "num_epochs = 10\r\n",
    "n_hidden1 = 50\r\n",
    "n_hidden2 = 100\r\n",
    "n_input = X_train.shape[1]\r\n",
    "n_samples = X_train.shape[0]\r\n",
    "n_classes = 1\r\n",
    "# trainのミニバッチイテレータ\r\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\r\n",
    "\r\n",
    "class MyModel(tf.keras.Model):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # 重みとバイアスの宣言\r\n",
    "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\r\n",
    "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\r\n",
    "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\r\n",
    "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\r\n",
    "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\r\n",
    "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\r\n",
    "    def call(self, x):\r\n",
    "        \"\"\"\r\n",
    "        単純な3層ニューラルネットワーク\r\n",
    "        \"\"\"\r\n",
    "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\r\n",
    "        layer_1 = tf.nn.relu(layer_1)\r\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\r\n",
    "        layer_2 = tf.nn.relu(layer_2)\r\n",
    "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\r\n",
    "        return layer_output\r\n",
    "model = MyModel()\r\n",
    "\r\n",
    "# # 最適化手法\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
    "\r\n",
    "def train(x, y):\r\n",
    "    logits = model(x, training=True)\r\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\r\n",
    "    return loss\r\n",
    "\r\n",
    "def evaluate(x, y):\r\n",
    "    logits = model(x)\r\n",
    "    loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\r\n",
    "    # 推定結果\r\n",
    "    correct_pred = tf.equal(tf.sign(y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\r\n",
    "    # 指標値計算\r\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
    "    return loss, accuracy\r\n",
    "\r\n",
    "# 計算グラフの実行\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    # エポックごとにループ\r\n",
    "    total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\r\n",
    "    # total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\r\n",
    "    total_loss = 0\r\n",
    "    total_acc = 0\r\n",
    "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\r\n",
    "        # ミニバッチごとにループ\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            loss = train(mini_batch_x, mini_batch_y)\r\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\r\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
    "        total_loss += loss\r\n",
    "    loss = total_loss / n_samples\r\n",
    "    val_loss, val_acc = evaluate(X_val, y_val)\r\n",
    "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\r\n",
    "_, test_acc = evaluate(X_test, y_test)\r\n",
    "print(\"test_acc : {:.3f}\".format(test_acc))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, loss : 10.0067, val_loss : 58.0180, val_acc : 0.375\n",
      "Epoch 1, loss : 4.3281, val_loss : 1.7625, val_acc : 0.750\n",
      "Epoch 2, loss : 0.9640, val_loss : 0.9367, val_acc : 0.938\n",
      "Epoch 3, loss : 0.3306, val_loss : 2.5875, val_acc : 0.750\n",
      "Epoch 4, loss : 0.0918, val_loss : 2.4758, val_acc : 0.812\n",
      "Epoch 5, loss : 0.3046, val_loss : 10.5018, val_acc : 0.438\n",
      "Epoch 6, loss : 0.4673, val_loss : 1.7533, val_acc : 0.875\n",
      "Epoch 7, loss : 0.3987, val_loss : 5.3787, val_acc : 0.688\n",
      "Epoch 8, loss : 0.4231, val_loss : 0.6651, val_acc : 0.938\n",
      "Epoch 9, loss : 0.2782, val_loss : 2.0339, val_acc : 0.750\n",
      "test_acc : 0.750\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "tensorflowを使った場合、前処理では特に変更はないが、入力層から中間層、中間層から出力層に至る部分が全て関数で用意されている。\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題3】3種類すべての目的変数を使用したIrisのモデルを作成\r\n",
    "Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類すべてを分類できるモデルを作成してください。<br>\r\n",
    "2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。<br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "\r\n",
    "# データセットの読み込み\r\n",
    "dataset_path =\"Iris.csv\"\r\n",
    "df = pd.read_csv(dataset_path)\r\n",
    "\r\n",
    "# データフレームから条件抽出\r\n",
    "y = df[\"Species\"]\r\n",
    "X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\r\n",
    "y = np.array(y)\r\n",
    "X = np.array(X).astype(np.float32)\r\n",
    "\r\n",
    "# ラベルを数値に変換\r\n",
    "y[y==\"Iris-setosa\"] = 0\r\n",
    "y[y==\"Iris-versicolor\"] = 1\r\n",
    "y[y==\"Iris-virginica\"] = 2\r\n",
    "y = y.astype(np.float32)[:, np.newaxis]\r\n",
    "\r\n",
    "# trainとtestに分割\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
    "# さらにtrainとvalに分割\r\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\r\n",
    "\r\n",
    "# 多値分類を行うため、one-hotエンコーディングを行う\r\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
    "y_train_one_hot = enc.fit_transform(y_train)\r\n",
    "y_val_one_hot = enc.transform(y_val)\r\n",
    "y_test_one_hot = enc.transform(y_test)\r\n",
    "\r\n",
    "# ミニバッチ処理\r\n",
    "class GetMiniBatch:\r\n",
    "    \"\"\"\r\n",
    "    ミニバッチを取得するイテレータ\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\r\n",
    "      訓練データ\r\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\r\n",
    "      正解値\r\n",
    "    batch_size : int\r\n",
    "      バッチサイズ\r\n",
    "    seed : int\r\n",
    "      NumPyの乱数のシード\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\r\n",
    "        self.batch_size = batch_size\r\n",
    "        np.random.seed(seed)\r\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
    "        self.X = X[shuffle_index]\r\n",
    "        self.y = y[shuffle_index]\r\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\r\n",
    "        # self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
    "    def __len__(self):\r\n",
    "        return self._stop\r\n",
    "    def __getitem__(self,item):\r\n",
    "        p0 = item*self.batch_size\r\n",
    "        p1 = item*self.batch_size + self.batch_size\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \r\n",
    "    def __iter__(self):\r\n",
    "        self._counter = 0\r\n",
    "        return self\r\n",
    "    def __next__(self):\r\n",
    "        if self._counter >= self._stop:\r\n",
    "            raise StopIteration()\r\n",
    "        p0 = self._counter*self.batch_size\r\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
    "        self._counter += 1\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]\r\n",
    "\r\n",
    "# ハイパーパラメータの設定\r\n",
    "learning_rate = 0.01\r\n",
    "batch_size = 10\r\n",
    "num_epochs = 10\r\n",
    "n_hidden1 = 50\r\n",
    "n_hidden2 = 100\r\n",
    "n_input = X_train.shape[1]\r\n",
    "n_samples = X_train.shape[0]\r\n",
    "n_classes = 3\r\n",
    "\r\n",
    "# trainのミニバッチイテレータ\r\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\r\n",
    "\r\n",
    "# メイン処理\r\n",
    "class MyModel(tf.keras.Model):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # 重みとバイアスの宣言\r\n",
    "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\r\n",
    "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\r\n",
    "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\r\n",
    "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\r\n",
    "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\r\n",
    "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\r\n",
    "    def call(self, x):\r\n",
    "        \"\"\"\r\n",
    "        単純な3層ニューラルネットワーク\r\n",
    "        \"\"\"\r\n",
    "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\r\n",
    "        layer_1 = tf.nn.relu(layer_1)\r\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\r\n",
    "        layer_2 = tf.nn.relu(layer_2)\r\n",
    "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\r\n",
    "        return layer_output\r\n",
    "\r\n",
    "# モデルのインスタンス化\r\n",
    "model = MyModel()\r\n",
    "\r\n",
    "# 最適化手法\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
    "\r\n",
    "def train(x, y):\r\n",
    "    logits = model(x, training=True)\r\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\r\n",
    "    # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\r\n",
    "    return loss\r\n",
    "\r\n",
    "def evaluate(x, y):\r\n",
    "    logits = model(x)\r\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\r\n",
    "    # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\r\n",
    "    # 推定結果\r\n",
    "    correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(logits, 1))\r\n",
    "    # correct_pred = tf.equal(tf.sign(y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\r\n",
    "    # 指標値計算\r\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
    "    return loss, accuracy\r\n",
    "\r\n",
    "# 計算グラフの実行\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    # エポックごとにループ\r\n",
    "    total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\r\n",
    "    # total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\r\n",
    "    total_loss = 0\r\n",
    "    total_acc = 0\r\n",
    "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\r\n",
    "        # ミニバッチごとにループ\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            loss = train(mini_batch_x, mini_batch_y)\r\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\r\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
    "        total_loss += loss\r\n",
    "    loss = total_loss / n_samples\r\n",
    "    val_loss, val_acc = evaluate(X_val, y_val)\r\n",
    "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\r\n",
    "_, test_acc = evaluate(X_test, y_test)\r\n",
    "print(\"test_acc : {:.3f}\".format(test_acc))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, loss : 7.0184, val_loss : 47.5524, val_acc : 0.667\n",
      "Epoch 1, loss : 4.8703, val_loss : 24.3688, val_acc : 0.375\n",
      "Epoch 2, loss : 3.4837, val_loss : 37.9057, val_acc : 0.542\n",
      "Epoch 3, loss : 5.4326, val_loss : 49.9296, val_acc : 0.792\n",
      "Epoch 4, loss : 5.8302, val_loss : 31.3906, val_acc : 0.500\n",
      "Epoch 5, loss : 4.6627, val_loss : 32.9556, val_acc : 0.667\n",
      "Epoch 6, loss : 5.0546, val_loss : 45.9969, val_acc : 0.667\n",
      "Epoch 7, loss : 6.1711, val_loss : 59.9783, val_acc : 1.000\n",
      "Epoch 8, loss : 7.3792, val_loss : 59.7534, val_acc : 0.833\n",
      "Epoch 9, loss : 7.6982, val_loss : 80.8442, val_acc : 1.000\n",
      "test_acc : 0.967\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題4】House Pricesのモデルを作成\r\n",
    "回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。<br>\r\n",
    "分類問題と回帰問題の違いを考慮してください。<br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "\r\n",
    "\r\n",
    "# データセットの読み込み\r\n",
    "dataset_path =r\"E:\\DiveIntoCode\\source\\train.csv\"\r\n",
    "df = pd.read_csv(dataset_path)\r\n",
    "\r\n",
    "# データフレームから条件抽出\r\n",
    "y = df[\"SalePrice\"]\r\n",
    "X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\r\n",
    "y = np.array(y)\r\n",
    "X = np.array(X).astype(np.float32)\r\n",
    "y = y.astype(int)[:, np.newaxis]\r\n",
    "# y = y.astype(np.int)[:, np.newaxis]\r\n",
    "y = np.log(y)\r\n",
    "\r\n",
    "# trainとtestに分割\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\r\n",
    "# さらにtrainとvalに分割\r\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\r\n",
    "\r\n",
    "# ミニバッチ処理\r\n",
    "class GetMiniBatch:\r\n",
    "    \"\"\"\r\n",
    "    ミニバッチを取得するイテレータ\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\r\n",
    "      訓練データ\r\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\r\n",
    "      正解値\r\n",
    "    batch_size : int\r\n",
    "      バッチサイズ\r\n",
    "    seed : int\r\n",
    "      NumPyの乱数のシード\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\r\n",
    "        self.batch_size = batch_size\r\n",
    "        np.random.seed(seed)\r\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
    "        self.X = X[shuffle_index]\r\n",
    "        self.y = y[shuffle_index]\r\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\r\n",
    "        # self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
    "    def __len__(self):\r\n",
    "        return self._stop\r\n",
    "    def __getitem__(self,item):\r\n",
    "        p0 = item*self.batch_size\r\n",
    "        p1 = item*self.batch_size + self.batch_size\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \r\n",
    "    def __iter__(self):\r\n",
    "        self._counter = 0\r\n",
    "        return self\r\n",
    "    def __next__(self):\r\n",
    "        if self._counter >= self._stop:\r\n",
    "            raise StopIteration()\r\n",
    "        p0 = self._counter*self.batch_size\r\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
    "        self._counter += 1\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]\r\n",
    "\r\n",
    "# ハイパーパラメータの設定\r\n",
    "learning_rate = 0.01\r\n",
    "batch_size = 10\r\n",
    "num_epochs = 10\r\n",
    "n_hidden1 = 50\r\n",
    "n_hidden2 = 100\r\n",
    "n_input = X_train.shape[1]\r\n",
    "n_samples = X_train.shape[0]\r\n",
    "n_classes = 1\r\n",
    "\r\n",
    "# trainのミニバッチイテレータ\r\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\r\n",
    "\r\n",
    "# メイン処理\r\n",
    "class MyModel(tf.keras.Model):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # 重みとバイアスの宣言\r\n",
    "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\r\n",
    "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\r\n",
    "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\r\n",
    "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\r\n",
    "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\r\n",
    "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\r\n",
    "    def call(self, x):\r\n",
    "        \"\"\"\r\n",
    "        単純な3層ニューラルネットワーク\r\n",
    "        \"\"\"\r\n",
    "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\r\n",
    "        layer_1 = tf.nn.relu(layer_1)\r\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\r\n",
    "        layer_2 = tf.nn.relu(layer_2)\r\n",
    "        layer_output = tf.matmul(layer_2, self.w3) + self.b3 \r\n",
    "        return layer_output\r\n",
    "\r\n",
    "# モデルのインスタンス化\r\n",
    "model = MyModel()\r\n",
    "\r\n",
    "# 最適化手法\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
    "\r\n",
    "def train(x, y):\r\n",
    "    logits = model(x, training=True)\r\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(y, logits))\r\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\r\n",
    "    # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\r\n",
    "    return loss\r\n",
    "\r\n",
    "def evaluate(x, y):\r\n",
    "    logits = model(x)\r\n",
    "    loss = tf.reduce_mean(tf.losses.mean_squared_error(y, logits))\r\n",
    "    # loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\r\n",
    "    # loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(y, logits))\r\n",
    "    # 推定結果\r\n",
    "    correct_pred = logits\r\n",
    "    # correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(logits, 1))\r\n",
    "    # correct_pred = tf.equal(tf.sign(y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\r\n",
    "    # 指標値計算\r\n",
    "    # accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
    "    # return loss, accuracy\r\n",
    "    return loss\r\n",
    "\r\n",
    "# 計算グラフの実行\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    # エポックごとにループ\r\n",
    "    total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\r\n",
    "    # total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\r\n",
    "    # total_loss = 0\r\n",
    "    # total_acc = 0\r\n",
    "    loss_list = []\r\n",
    "    val_loss_list = []\r\n",
    "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            loss = train(mini_batch_x, mini_batch_y)\r\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\r\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
    "        # total_loss += loss\r\n",
    "    loss = evaluate(X_train, y_train)\r\n",
    "    loss_list.append(loss)\r\n",
    "    val_loss = evaluate(X_val, y_val)\r\n",
    "    val_loss_list.append(val_loss)\r\n",
    "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, loss, val_loss))\r\n",
    "#     loss = total_loss / n_samples\r\n",
    "#     val_loss, val_acc = evaluate(X_val, y_val)\r\n",
    "#     print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\r\n",
    "# _, test_acc = evaluate(X_test, y_test)\r\n",
    "# print(\"test_acc : {:.3f}\".format(test_acc))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 0, loss : 946561.3750, val_loss : 660438.3125\n",
      "Epoch 1, loss : 422990.5938, val_loss : 237097.0312\n",
      "Epoch 2, loss : 235538.0312, val_loss : 149774.0469\n",
      "Epoch 3, loss : 115220.8281, val_loss : 56409.1250\n",
      "Epoch 4, loss : 109251.3438, val_loss : 71649.3984\n",
      "Epoch 5, loss : 102866.2266, val_loss : 83198.4297\n",
      "Epoch 6, loss : 49001.7812, val_loss : 36705.3711\n",
      "Epoch 7, loss : 41651.2969, val_loss : 29357.2031\n",
      "Epoch 8, loss : 39284.7734, val_loss : 32028.3359\n",
      "Epoch 9, loss : 26814.8398, val_loss : 22660.3711\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題5】MNISTのモデルを作成\r\n",
    "ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。<br>\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "import tensorflow as tf\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from tensorflow.keras.datasets import mnist\r\n",
    "\r\n",
    "# データを分割\r\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
    "\r\n",
    "# 平滑化\r\n",
    "X_train = X_train.reshape(-1, 784)\r\n",
    "X_test = X_test.reshape(-1, 784)\r\n",
    "\r\n",
    "# 0.0～1.0の間に正規化\r\n",
    "X_train = X_train.astype(float)\r\n",
    "X_test = X_test.astype(float)\r\n",
    "X_train /= 255\r\n",
    "X_test /= 255\r\n",
    "\r\n",
    "# one-hotエンコード\r\n",
    "y_train = y_train.astype(np.int)[:, np.newaxis]\r\n",
    "y_test = y_test.astype(np.int)[:, np.newaxis]\r\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
    "y_train_one_hot = enc.fit_transform(y_train[:])\r\n",
    "y_test_one_hot = enc.fit_transform(y_test[:])\r\n",
    "\r\n",
    "# trainデータを更に分割\r\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\r\n",
    "\r\n",
    "# ミニバッチ処理\r\n",
    "class GetMiniBatch:\r\n",
    "    \"\"\"\r\n",
    "    ミニバッチを取得するイテレータ\r\n",
    "    Parameters\r\n",
    "    ----------\r\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\r\n",
    "      訓練データ\r\n",
    "    y : 次の形のndarray, shape (n_samples, 1)\r\n",
    "      正解値\r\n",
    "    batch_size : int\r\n",
    "      バッチサイズ\r\n",
    "    seed : int\r\n",
    "      NumPyの乱数のシード\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, X, y, batch_size = 10, seed=0):\r\n",
    "        self.batch_size = batch_size\r\n",
    "        np.random.seed(seed)\r\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
    "        self.X = X[shuffle_index]\r\n",
    "        self.y = y[shuffle_index]\r\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\r\n",
    "        # self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\r\n",
    "    def __len__(self):\r\n",
    "        return self._stop\r\n",
    "    def __getitem__(self,item):\r\n",
    "        p0 = item*self.batch_size\r\n",
    "        p1 = item*self.batch_size + self.batch_size\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]        \r\n",
    "    def __iter__(self):\r\n",
    "        self._counter = 0\r\n",
    "        return self\r\n",
    "    def __next__(self):\r\n",
    "        if self._counter >= self._stop:\r\n",
    "            raise StopIteration()\r\n",
    "        p0 = self._counter*self.batch_size\r\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
    "        self._counter += 1\r\n",
    "        return self.X[p0:p1], self.y[p0:p1]\r\n",
    "\r\n",
    "# ハイパーパラメータの設定\r\n",
    "learning_rate = 0.01\r\n",
    "batch_size = 10\r\n",
    "num_epochs = 10\r\n",
    "n_hidden1 = 50\r\n",
    "n_hidden2 = 100\r\n",
    "n_input = X_train.shape[1]\r\n",
    "n_samples = X_train.shape[0]\r\n",
    "n_classes = 10\r\n",
    "\r\n",
    "# trainのミニバッチイテレータ\r\n",
    "get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\r\n",
    "\r\n",
    "# メイン処理\r\n",
    "class MyModel(tf.keras.Model):\r\n",
    "    def __init__(self):\r\n",
    "        super().__init__()\r\n",
    "        # 重みとバイアスの宣言\r\n",
    "        self.w1 = tf.Variable(tf.random.normal([n_input, n_hidden1]), trainable=True)\r\n",
    "        self.w2 = tf.Variable(tf.random.normal([n_hidden1, n_hidden2]), trainable=True)\r\n",
    "        self.w3 = tf.Variable(tf.random.normal([n_hidden2, n_classes]), trainable=True)\r\n",
    "        self.b1 = tf.Variable(tf.random.normal([n_hidden1]), trainable=True)\r\n",
    "        self.b2 = tf.Variable(tf.random.normal([n_hidden2]), trainable=True)\r\n",
    "        self.b3 = tf.Variable(tf.random.normal([n_classes]), trainable=True)\r\n",
    "    def call(self, x):\r\n",
    "        \"\"\"\r\n",
    "        単純な3層ニューラルネットワーク\r\n",
    "        \"\"\"\r\n",
    "        layer_1 = tf.add(tf.matmul(x, self.w1), self.b1)\r\n",
    "        layer_1 = tf.nn.relu(layer_1)\r\n",
    "        layer_2 = tf.add(tf.matmul(layer_1, self.w2), self.b2)\r\n",
    "        layer_2 = tf.nn.relu(layer_2)\r\n",
    "        layer_output = tf.matmul(layer_2, self.w3) + self.b3  # tf.addと+は等価である\r\n",
    "        return layer_output\r\n",
    "\r\n",
    "# モデルのインスタンス化\r\n",
    "model = MyModel()\r\n",
    "\r\n",
    "# 最適化手法\r\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate)\r\n",
    "\r\n",
    "def train(x, y):\r\n",
    "    logits = model(x, training=True)\r\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\r\n",
    "    return loss\r\n",
    "\r\n",
    "def evaluate(x, y):\r\n",
    "    logits = model(x)\r\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(y, logits))\r\n",
    "    # 推定結果\r\n",
    "    correct_pred = tf.equal(tf.argmax(y, 1), tf.argmax(logits, 1))\r\n",
    "    # correct_pred = tf.equal(tf.sign(y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\r\n",
    "    # 指標値計算\r\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\r\n",
    "    return loss, accuracy\r\n",
    "\r\n",
    "# 計算グラフの実行\r\n",
    "for epoch in range(num_epochs):\r\n",
    "    # エポックごとにループ\r\n",
    "    total_batch = np.ceil(X_train.shape[0]/batch_size).astype(int)\r\n",
    "    # total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\r\n",
    "    total_loss = 0\r\n",
    "    total_acc = 0\r\n",
    "    for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\r\n",
    "        # ミニバッチごとにループ\r\n",
    "        with tf.GradientTape() as tape:\r\n",
    "            loss = train(mini_batch_x, mini_batch_y)\r\n",
    "        grads = tape.gradient(loss, model.trainable_weights)\r\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\r\n",
    "        total_loss += loss\r\n",
    "    loss = total_loss / n_samples\r\n",
    "    val_loss, val_acc = evaluate(X_val, y_val)\r\n",
    "    print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, val_acc))\r\n",
    "_, test_acc = evaluate(X_test, y_test)\r\n",
    "print(\"test_acc : {:.3f}\".format(test_acc))\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "D:\\anaconda3\\envs\\DIC06env\\lib\\site-packages\\ipykernel_launcher.py:22: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "D:\\anaconda3\\envs\\DIC06env\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "WARNING:tensorflow:Layer my_model_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because its dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "Epoch 0, loss : 0.7207, val_loss : 1.0135, val_acc : 0.782\n",
      "Epoch 1, loss : 0.0730, val_loss : 0.5310, val_acc : 0.875\n",
      "Epoch 2, loss : 0.0377, val_loss : 0.3456, val_acc : 0.918\n",
      "Epoch 3, loss : 0.0296, val_loss : 0.3348, val_acc : 0.918\n",
      "Epoch 4, loss : 0.0276, val_loss : 0.3175, val_acc : 0.927\n",
      "Epoch 5, loss : 0.0256, val_loss : 0.2832, val_acc : 0.939\n",
      "Epoch 6, loss : 0.0240, val_loss : 0.3099, val_acc : 0.937\n",
      "Epoch 7, loss : 0.0233, val_loss : 0.3394, val_acc : 0.935\n",
      "Epoch 8, loss : 0.0224, val_loss : 0.3438, val_acc : 0.935\n",
      "Epoch 9, loss : 0.0225, val_loss : 0.3233, val_acc : 0.943\n",
      "test_acc : 0.099\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.11 64-bit ('DIC06env': conda)"
  },
  "interpreter": {
   "hash": "ba7d5cb763a6d7026e371a12292fd63561295f5716840e1bef2c60ae7fe0ad24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
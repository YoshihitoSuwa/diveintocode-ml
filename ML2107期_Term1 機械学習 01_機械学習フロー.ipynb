{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1068cc",
   "metadata": {},
   "source": [
    "## 【問題1】クロスバリデーション\n",
    "事前学習期間は検証データを分割しておき、それに対して指標値を計算することで検証を行っていました。しかし、分割の仕方により精度は変化します。実践的には クロスバリデーション を行います。<br>\n",
    "具体的には分割を複数回行い、それぞれに対して学習と検証を行う方法です。複数回の分割を行う関数はscikit-learnにKFoldとして用意されています。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91244325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor # 回帰用\n",
    "from sklearn.ensemble import RandomForestClassifier # 分類用\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "\n",
    "# ファイル名（パス）を指定する\n",
    "csv_path = \"E:/DiveIntoCode/source/application_train.csv\" # 絶対パス\n",
    "\n",
    "# 指数表示の禁止を設定する ※numpyのみなので注意\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# csvファイル読み込み ※該当のcsvの１行目が列名となっているため、列名を指定したりせず、そのまま読み込む\n",
    "df_train = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "06c805a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 61503  61504  61505 ... 307508 307509 307510] TEST: [    0     1     2 ... 61500 61501 61502]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 61503  61504  61505 ... 123002 123003 123004]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [123005 123006 123007 ... 184504 184505 184506]\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [184507 184508 184509 ... 246006 246007 246008]\n",
      "TRAIN: [     0      1      2 ... 246006 246007 246008] TEST: [246009 246010 246011 ... 307508 307509 307510]\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# Xに説明変数、yに目的変数となる列をそれぞれ抽出して代入\n",
    "df_X = df_train[[\"AMT_CREDIT\",\"DAYS_EMPLOYED\"]]\n",
    "df_y = df_train[\"TARGET\"]\n",
    "# X,yをndarrayへ変換\n",
    "X = df_X.values\n",
    "y = df_y.values\n",
    "\n",
    "# test_splitにX、yを代入（デフォルト(指定なし)で訓練75%、検証25%となる）\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "# indexをランダムに分ける関数の模様\n",
    "# n_splits=? の?個にデータを分割(全データ÷?)する。検証とテストデータの割合は分割する数による ５分割であれば４(検証)：１(テスト)\n",
    "# 結果、分割されたデータ群はテストデータにそれぞれ１回ずつなる\n",
    "kf = KFold(n_splits=5)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # インスタンス化\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # 訓練用のデータのみを.fitで標準化\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # 訓練用、検証用双方のデータにtransformで標準化\n",
    "    tr_X_tarin = scaler.transform(X_train)\n",
    "    tr_X_test = scaler.transform(X_test)\n",
    "\n",
    "    # 学習（ランダムフォレスト）\n",
    "    regr = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    regr.fit(tr_X_tarin, y_train)\n",
    "\n",
    "# 学習モデルによる推定\n",
    "Y_pred = regr.predict(tr_X_test)\n",
    "\n",
    "# ROC曲線とAUCの出力\n",
    "fpr, tpr, thresholds = roc_curve(y_test, Y_pred)\n",
    "result_auc = auc(fpr, tpr)\n",
    "\n",
    "print(result_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87939edb",
   "metadata": {},
   "source": [
    "## 【問題2】グリッドサーチ\n",
    "これまで分類器のパラメータは基本的にデフォルトの設定を使用していました。パラメータの詳細は今後のSprintで学んでいくことになりますが、パラメータは状況に応じて最適なものを選ぶ必要があります。パラメータを探索するために グリッドサーチ と呼ばれる総当たり的手法が一般的に利用されます。<br>\n",
    "グリッドサーチをパイプラインの中に組み込みましょう。<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f0938a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor # 回帰用\n",
    "from sklearn.ensemble import RandomForestClassifier # 分類用\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# ファイル名（パス）を指定する\n",
    "csv_path = \"E:/DiveIntoCode/source/application_train.csv\" # 絶対パス\n",
    "\n",
    "# 指数表示の禁止を設定する ※numpyのみなので注意\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# csvファイル読み込み ※該当のcsvの１行目が列名となっているため、列名を指定したりせず、そのまま読み込む\n",
    "df_train = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "548fdc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [153756 153757 153758 ... 307508 307509 307510] TEST: [     0      1      2 ... 153753 153754 153755]\n",
      "1\n",
      "0.49999291859929895\n",
      "TRAIN: [     0      1      2 ... 153753 153754 153755] TEST: [153756 153757 153758 ... 307508 307509 307510]\n",
      "2\n",
      "0.4999964657067526\n"
     ]
    }
   ],
   "source": [
    "# Xに説明変数、yに目的変数となる列をそれぞれ抽出して代入\n",
    "df_X = df_train[[\"AMT_CREDIT\",\"DAYS_EMPLOYED\"]]\n",
    "df_y = df_train[\"TARGET\"]\n",
    "# X,yをndarrayへ変換\n",
    "X = df_X.values\n",
    "y = df_y.values\n",
    "\n",
    "### ランダムフォレストで使用するパラメータ\n",
    "# n_estimators 10, 50, 100, 150, 200\n",
    "# max_depth : 2, 4, 6, 8, 10\n",
    "# min_samples_split : 1, 3, 5, 7, 9\n",
    "# max_leaf_nodes : 5, 10, 15, 20, 25\n",
    "# min_samples_leaf : 2, 4, 6, 8, 10\n",
    "# max_sample（ブートストラップサンプル）\n",
    "# max_features\n",
    "\n",
    "#グリッドサーチ対象のハイパーパラメーターを準備\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 50, 100, 150, 200],\n",
    "#     'max_depth': [2, 4, 6, 8, 10],\n",
    "#     'min_samples_split': [1, 3, 5, 7, 9],\n",
    "#     'min_samples_leaf': [2, 4, 6, 8, 10],\n",
    "#     'max_leaf_nodes': [5, 10, 15, 20, 25]\n",
    "#     }\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [2, 10],\n",
    "    }\n",
    "\n",
    "roop_cnt = 0\n",
    "\n",
    "# クロスバリデーション\n",
    "n_num = 2 # データの分割数\n",
    "\n",
    "kf = KFold(n_splits=n_num)\n",
    "for train_index, test_index in kf.split(df_X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # インスタンス化\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # 訓練用のデータのみを.fitで標準化\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # 訓練用、検証用双方のデータにtransformで標準化\n",
    "    tr_X_tarin = scaler.transform(X_train)\n",
    "    tr_X_test = scaler.transform(X_test)\n",
    "\n",
    "    # グリッドサーチ\n",
    "    forest_grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                               param_grid = param_grid,\n",
    "                               n_jobs = 1,\n",
    "                               cv = 5,\n",
    "                               scoring = \"roc_auc\"\n",
    "                              )\n",
    "    forest_grid.fit(tr_X_tarin, y_train)\n",
    "\n",
    "    roop_cnt += 1\n",
    "    print(roop_cnt)\n",
    "\n",
    "    # 学習モデルによる推定\n",
    "    Y_pred = forest_grid.predict(tr_X_test)\n",
    "\n",
    "    # ROC曲線とAUCの出力\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, Y_pred)\n",
    "    result_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(result_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e5a2d8",
   "metadata": {},
   "source": [
    "## 【問題3】Kernelからの調査\n",
    "KaggleのKernelから自身にはなかったアイデアを見つけ出して、列挙してください。そして、効果があると考えられるものを検証してください。<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b535c4d0",
   "metadata": {},
   "source": [
    "メモリの節約：速度面では処理が増えた分マイナスになるが、処理で使用するメモリを省略することでより大規模なデータに対応する際に効果が期待できそう\n",
    "\n",
    "```\n",
    "def reduce_mem_usage(df_):\n",
    "    start_mem = df_.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe: {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for c in df_.columns[df_.dtypes != 'object']:\n",
    "        col_type = df_[c].dtype\n",
    "        \n",
    "        c_min = df_[c].min()\n",
    "        c_max = df_[c].max()\n",
    "        if str(col_type)[:3] == 'int':\n",
    "            if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                df_[c] = df_[c].astype(np.int8)\n",
    "            elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                df_[c] = df_[c].astype(np.int16)\n",
    "            elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                df_[c] = df_[c].astype(np.int32)\n",
    "            else:\n",
    "                df_[c] = df_[c].astype(np.int64)  \n",
    "        else:\n",
    "            if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                df_[c] = df_[c].astype(np.float16)\n",
    "            elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                df_[c] = df_[c].astype(np.float32)\n",
    "            else:\n",
    "                df_[c] = df_[c].astype(np.float64)\n",
    "\n",
    "    end_mem = df_.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df_\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff202053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 61503  61504  61505 ... 307508 307509 307510] TEST: [    0     1     2 ... 61500 61501 61502]\n",
      "1\n",
      "0.5\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [ 61503  61504  61505 ... 123002 123003 123004]\n",
      "2\n",
      "0.5\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [123005 123006 123007 ... 184504 184505 184506]\n",
      "3\n",
      "0.5\n",
      "TRAIN: [     0      1      2 ... 307508 307509 307510] TEST: [184507 184508 184509 ... 246006 246007 246008]\n",
      "4\n",
      "0.5\n",
      "TRAIN: [     0      1      2 ... 246006 246007 246008] TEST: [246009 246010 246011 ... 307508 307509 307510]\n",
      "5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor # 回帰用\n",
    "from sklearn.ensemble import RandomForestClassifier # 分類用\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# ファイル名（パス）を指定する\n",
    "csv_path = \"E:/DiveIntoCode/source/application_train.csv\" # 絶対パス\n",
    "\n",
    "# 指数表示の禁止を設定する ※numpyのみなので注意\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "# csvファイル読み込み ※該当のcsvの１行目が列名となっているため、列名を指定したりせず、そのまま読み込む\n",
    "df_train = pd.read_csv(csv_path)\n",
    "\n",
    "# Xに説明変数、yに目的変数となる列をそれぞれ抽出して代入\n",
    "df_X = df_train[[\"AMT_CREDIT\",\"DAYS_EMPLOYED\"]]\n",
    "df_y = df_train[\"TARGET\"]\n",
    "# X,yをndarrayへ変換\n",
    "X = df_X.values\n",
    "y = df_y.values\n",
    "\n",
    "### ランダムフォレストで使用するパラメータ\n",
    "# n_estimators 10, 50, 100, 150, 200\n",
    "# max_depth : 2, 4, 6, 8, 10\n",
    "# min_samples_split : 1, 3, 5, 7, 9\n",
    "# max_leaf_nodes : 5, 10, 15, 20, 25\n",
    "# min_samples_leaf : 2, 4, 6, 8, 10\n",
    "# max_sample（ブートストラップサンプル）\n",
    "# max_features\n",
    "\n",
    "#グリッドサーチ対象のハイパーパラメーターを準備\n",
    "# param_grid = {\n",
    "#     'n_estimators': [10, 50, 100, 150, 200],\n",
    "#     'max_depth': [2, 4, 6, 8, 10],\n",
    "#     'min_samples_split': [1, 3, 5, 7, 9],\n",
    "#     'min_samples_leaf': [2, 4, 6, 8, 10],\n",
    "#     'max_leaf_nodes': [5, 10, 15, 20, 25]\n",
    "#     }\n",
    "param_grid = {\n",
    "    'n_estimators': [100],\n",
    "    'max_depth': [6],\n",
    "    'min_samples_split': [5],\n",
    "    'min_samples_leaf': [6],\n",
    "    'max_leaf_nodes': [15]\n",
    "    }\n",
    "\n",
    "roop_cnt = 0\n",
    "\n",
    "# クロスバリデーション\n",
    "n_num = 5 # データの分割数\n",
    "\n",
    "kf = KFold(n_splits=n_num)\n",
    "for train_index, test_index in kf.split(df_X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    # インスタンス化\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # 訓練用のデータのみを.fitで標準化\n",
    "    scaler.fit(X_train)\n",
    "\n",
    "    # 訓練用、検証用双方のデータにtransformで標準化\n",
    "    tr_X_tarin = scaler.transform(X_train)\n",
    "    tr_X_test = scaler.transform(X_test)\n",
    "\n",
    "    # グリッドサーチ\n",
    "    forest_grid = GridSearchCV(estimator=RandomForestClassifier(random_state=42),\n",
    "                               param_grid = param_grid,\n",
    "                               n_jobs = 1,\n",
    "                               cv = 5,\n",
    "                               scoring = \"roc_auc\"\n",
    "                              )\n",
    "    forest_grid.fit(tr_X_tarin, y_train)\n",
    "\n",
    "    roop_cnt += 1\n",
    "    print(roop_cnt)\n",
    "\n",
    "    # 学習モデルによる推定\n",
    "    Y_pred = forest_grid.predict(tr_X_test)\n",
    "\n",
    "    # ROC曲線とAUCの出力\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, Y_pred)\n",
    "    result_auc = auc(fpr, tpr)\n",
    "\n",
    "    print(result_auc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34b9610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

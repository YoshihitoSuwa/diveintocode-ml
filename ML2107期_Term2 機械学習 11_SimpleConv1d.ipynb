{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from keras.datasets import mnist\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成\n",
    "### 【問題2】1次元畳み込み後の出力サイズの計算\n",
    "### 【問題3】小さな配列での1次元畳み込み層の実験"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "x = np.array([1,2,3,4])\r\n",
    "w = np.array([3, 5, 7])\r\n",
    "b = np.array([1])\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# print(sum(x[0:3] * w)+b)\r\n",
    "# print(sum(x[1:4] * w)+b)\r\n",
    "\r\n",
    "a = []\r\n",
    "for s in range(len(w) - 1):\r\n",
    "    a.append((x[s:w.shape[0]+s] @ w) + b)\r\n",
    "\r\n",
    "print(a)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[array([35]), array([50])]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "n_out = 0 # 求める値。出力のサイズ（特徴量の数）\r\n",
    "n_in = x.shape[0] # 入力のサイズ（特徴量の数）\r\n",
    "p = 1 # ある方向へのパディング\r\n",
    "f = 1 # フィルタのサイズ\r\n",
    "s = 1 # ストライドのサイズ\r\n",
    "\r\n",
    "n_out = ((n_in + 2 * p - f) / s ) + 1\r\n",
    "print(n_out)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "delta_a = np.array([10, 20])\r\n",
    "\r\n",
    "delta_b = np.sum(delta_a)\r\n",
    "a = []\r\n",
    "# c = []\r\n",
    "for i in range(len(w)):\r\n",
    "    a.append(delta_a @ x[i:len(delta_a)+i])\r\n",
    "#    c.append(sum(delta_a * x[i:len(delta_a)+i])) # 計算結果として上と同じになる。意味的にはこちら\r\n",
    "delta_w = np.array(a)\r\n",
    "# delta_w1 = np.array(c)\r\n",
    "# print(c)\r\n",
    "\r\n",
    "# ここ(バックプロパゲーション)の処理を確認\r\n",
    "# b = []\r\n",
    "# new_w = np.insert(w[::-1], 0, 0)\r\n",
    "# new_w = np.append(new_w, 0)\r\n",
    "# for s in range(len(new_w) -1):\r\n",
    "#     b.append(new_w[s:len(delta_a)+s] @ delta_a)\r\n",
    "# delta_x = np.array(b[::-1])\r\n",
    "\r\n",
    "b = []\r\n",
    "print(len(delta_w))\r\n",
    "print(delta_w)\r\n",
    "for da_cnt_1 in range(len(delta_a)):\r\n",
    "    for w_cnt_1 in range(len(delta_w)):\r\n",
    "        if (da_cnt_1 == 0) & (w_cnt_1 == 0):\r\n",
    "            b.append(delta_a[da_cnt_1] * w[w_cnt_1])\r\n",
    "        elif da_cnt_1 == 1:\r\n",
    "            b.append(delta_a[da_cnt_1] * w[len(delta_w)-1])\r\n",
    "            break\r\n",
    "        else:\r\n",
    "            b.append(\r\n",
    "                (delta_a[da_cnt_1] * w[w_cnt_1]) \r\n",
    "                + \r\n",
    "                (delta_a[da_cnt_1 + 1] * w[w_cnt_1 - 1]))\r\n",
    "delta_x = np.array(b)\r\n",
    "\r\n",
    "print(\"delta_b:{}\".format(delta_b))\r\n",
    "print(\"delta_w:{}\".format(delta_w))\r\n",
    "print(\"delta_x:{}\".format(delta_x))\r\n",
    "print(\"delta_x.shape:{}\".format(delta_x.shape))\r\n",
    "\r\n",
    "print(\"delta_x[0]:{}\".format(delta_a[0] * w[0]))\r\n",
    "print(\"delta_x[1]:{}\".format((delta_a[0] * w[1]) + (delta_a[1] * w[0])))\r\n",
    "print(\"delta_x[2]:{}\".format((delta_a[0] * w[2]) + (delta_a[1] * w[1])))\r\n",
    "print(\"delta_x[3]:{}\".format(delta_a[1] * w[2]))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3\n",
      "[ 50  80 110]\n",
      "delta_b:30\n",
      "delta_w:[ 50  80 110]\n",
      "delta_x:[ 30 110 170 140]\n",
      "delta_x.shape:(4,)\n",
      "delta_x[0]:30\n",
      "delta_x[1]:110\n",
      "delta_x[2]:170\n",
      "delta_x[3]:140\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def output_size_calculation(n_in, F, P=0, S=1):\r\n",
    "    n_out = int((n_in + 2*P - F) / S + 1)\r\n",
    "    return n_out\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class SimpleConv1d():\r\n",
    "\r\n",
    "    def forward(self, x, w, b):\r\n",
    "        a = []\r\n",
    "        for i in range(len(w) - 1):\r\n",
    "            a.append((x[i:i+len(w)] @ w) + b[0])\r\n",
    "        return np.array(a)\r\n",
    "    \r\n",
    "    def backward(self, x, w, da):\r\n",
    "        db = np.sum(da)\r\n",
    "        dw = []\r\n",
    "        for i in range(len(w)):\r\n",
    "            dw.append(da @ x[i:i+len(da)])\r\n",
    "        dw = np.array(dw)\r\n",
    "        dx = []\r\n",
    "        new_w = np.insert(w[::-1], 0, 0)\r\n",
    "        new_w = np.append(new_w, 0)\r\n",
    "        for i in range(len(new_w)-1):\r\n",
    "            dx.append(new_w[i:i+len(da)] @ da)\r\n",
    "        dx = np.array(dx[::-1])\r\n",
    "        return db, dw, dx\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class GetMiniBatch:\r\n",
    "\r\n",
    "    def __init__(self, X, y, batch_size = 20, seed=0):\r\n",
    "        self.batch_size = batch_size\r\n",
    "        np.random.seed(seed)\r\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\r\n",
    "        self._X = X[shuffle_index]\r\n",
    "        self._y = y[shuffle_index]\r\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(int)\r\n",
    "        \r\n",
    "    def __len__(self):\r\n",
    "        return self._stop\r\n",
    "    \r\n",
    "    def __getitem__(self,item):\r\n",
    "        p0 = item*self.batch_size\r\n",
    "        p1 = item*self.batch_size + self.batch_size\r\n",
    "        return self._X[p0:p1], self._y[p0:p1] \r\n",
    "    \r\n",
    "    def __iter__(self):\r\n",
    "        self._counter = 0\r\n",
    "        return self\r\n",
    "    \r\n",
    "    def __next__(self):\r\n",
    "        if self._counter >= self._stop:\r\n",
    "            raise StopIteration()\r\n",
    "        p0 = self._counter*self.batch_size\r\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\r\n",
    "        self._counter += 1\r\n",
    "        return self._X[p0:p1], self._y[p0:p1]\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "class SimpleInitializer:\r\n",
    "\r\n",
    "    def __init__(self, sigma):\r\n",
    "        self.sigma = sigma\r\n",
    "        \r\n",
    "    def W(self, *shape):\r\n",
    "        W = self.sigma * np.random.randn(*shape)\r\n",
    "        return W\r\n",
    "    \r\n",
    "    def B(self, *shape):\r\n",
    "        B = self.sigma * np.random.randn(*shape)\r\n",
    "        return B\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class SGD:\r\n",
    "    \r\n",
    "    def __init__(self, lr):\r\n",
    "        self.lr = lr\r\n",
    "        \r\n",
    "    def update(self, layer):\r\n",
    "        layer.W -= self.lr * layer.dW / len(layer.Z)\r\n",
    "        layer.B -= self.lr * layer.dB / len(layer.Z)\r\n",
    "        return layer\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class AdaGrad:\r\n",
    "\r\n",
    "    def __init__(self, lr):\r\n",
    "        self.lr = lr \r\n",
    "    \r\n",
    "    def update(self, layer):\r\n",
    "        layer.HW += layer.dW * layer.dW\r\n",
    "        layer.HB += layer.dB * layer.dB\r\n",
    "        delta = 1e-7 \r\n",
    "        layer.W -= self.lr * layer.dW / (np.sqrt(layer.HW) + delta) / len(layer.Z)\r\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(layer.HB) + delta) / len(layer.Z)\r\n",
    "        return layer\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "class softmax:\r\n",
    "\r\n",
    "    def forward(self, A): \r\n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\r\n",
    "        return Z\r\n",
    "        \r\n",
    "    def backward(self, Z, y):\r\n",
    "        dA = Z - y\r\n",
    "        loss = - np.sum(y * np.log(Z)) / len(y)\r\n",
    "        return dA, loss\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class Conv1d:\r\n",
    "    \r\n",
    "    def __init__(self,\r\n",
    "                 b_size,\r\n",
    "                 initializer,\r\n",
    "                 optimizer,\r\n",
    "                 n_in_channels=1,\r\n",
    "                 n_out_channels=1,\r\n",
    "                 pa=0):\r\n",
    "        self.b_size = b_size\r\n",
    "        self.optimizer = optimizer\r\n",
    "        self.pa = pa\r\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\r\n",
    "        # self.W = initializer.W(n_out_channels, n_in_channels)\r\n",
    "        self.B = initializer.B(n_out_channels)\r\n",
    "        self.n_in_channels = n_in_channels\r\n",
    "        self.n_out_channels = n_out_channels\r\n",
    "        self.n_out = None\r\n",
    "        \r\n",
    "    def forward(self, X):\r\n",
    "        #Forward Propagation\r\n",
    "        # print(\"==================================================\")\r\n",
    "        # print(\"X:{}\".format(X))\r\n",
    "        self.n_in = X.shape[-1]\r\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa)\r\n",
    "        X = X.reshape(self.n_in_channels, self.n_in)\r\n",
    "        self.X = np.pad(X, ((0,0), ((self.b_size-1), 0)))\r\n",
    "        self.X1 = np.zeros((self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\r\n",
    "        # print(\"self.n_in:{}\".format(self.n_in))\r\n",
    "        # print(\"self.n_out:{}\".format(self.n_out))\r\n",
    "        # print(\"X:{}\".format(X))\r\n",
    "        # print(\"self.X:{}\".format(self.X))\r\n",
    "        for i in range(self.b_size):\r\n",
    "            self.X1[:, i] = np.roll(self.X, -i, axis=-1)\r\n",
    "            # print(\"self.X1[:, i]:{}\".format(self.X1[:, i]))\r\n",
    "        # print(\"==================================================\")\r\n",
    "        # print(\"self.X1:{}\".format(self.X1))\r\n",
    "        # print(\"self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa].shape:{}\".format((self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]).shape))\r\n",
    "        # print(\"self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]:{}\".format(self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]))\r\n",
    "        # print(\"self.W[:, :, :, np.newaxis].shape:{}\".format((self.W[:, :, :, np.newaxis]).shape))\r\n",
    "        # print(\"self.W[:, :, :, np.newaxis]:{}\".format(self.W[:, :, :, np.newaxis]))\r\n",
    "        A = np.sum(self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa]*self.W[:, :, :, np.newaxis], axis=(1, 2)) + self.B.reshape(-1,1)\r\n",
    "        # print(\"==================================================\")\r\n",
    "        # print(\"A:{}\".format(A))\r\n",
    "        return A\r\n",
    "    \r\n",
    "    def backward(self, dA):\r\n",
    "        #Back Propagation\r\n",
    "        self.dW = np.sum(np.dot(dA, self.X1[:, :, self.b_size-1-self.pa:self.n_in+self.pa, np.newaxis]), axis=-1)\r\n",
    "        self.dB = np.sum(dA, axis=1)\r\n",
    "        self.dA = np.pad(dA, ((0,0), (0, (self.b_size-1))))\r\n",
    "        self.dA1 = np.zeros((self.n_out_channels, self.b_size, self.dA.shape[-1]))\r\n",
    "        for i in range(self.b_size):\r\n",
    "            self.dA1[:, i] = np.roll(self.dA, i, axis=-1)\r\n",
    "        dX = np.sum(self.W@self.dA1, axis=0)\r\n",
    "        self.optimizer.update(self)\r\n",
    "        return dX\r\n",
    "\r\n",
    "    def output_size_calculation(n_in, F, P=0, S=1):\r\n",
    "        n_out = int((n_in + 2*P - F) / S + 1)\r\n",
    "        return n_out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\"\"\"\r\n",
    "b_size : フィルターサイズ\r\n",
    "initializer : 初期化クラス\r\n",
    "optimizer : 最適化手法クラス\r\n",
    "n_in_channels : 入力チャンネル数\r\n",
    "n_out_channels : 出力チャンネル数\r\n",
    "pa : パディング数\r\n",
    "\"\"\"\r\n",
    "test = Conv1d(b_size=3,\r\n",
    "              initializer=SimpleInitializer(0.01),\r\n",
    "              optimizer=SGD(0.01),\r\n",
    "              n_in_channels=2,\r\n",
    "              n_out_channels=3,\r\n",
    "              pa=0)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) # shape(2, 4)で、（入力チャンネル数、特徴量数）である。\r\n",
    "# w = np.ones((3, 2, 3)) # 例の簡略化のため全て1とする。(出力チャンネル数、入力チャンネル数、フィルタサイズ)である。\r\n",
    "# b = np.array([1, 2, 3]) # （出力チャンネル数）\r\n",
    "\r\n",
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]]) \r\n",
    "test.W = np.ones((3, 2, 3), dtype=float)\r\n",
    "test.B = np.array([1, 2, 3], dtype=float)\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "a = test.forward(x)\r\n",
    "print(a)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "【問題8】学習と推定"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "from keras.datasets import mnist\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "from sklearn.metrics import precision_score\r\n",
    "from sklearn.metrics import recall_score\r\n",
    "from sklearn.metrics import f1_score\r\n",
    "from sklearn.metrics import confusion_matrix\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from sklearn.preprocessing import OneHotEncoder\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "import numpy as np\r\n",
    "import math\r\n",
    "\r\n",
    "\r\n",
    "class Sigmoid:\r\n",
    "    \r\n",
    "    def forward(self, A):\r\n",
    "        self.A = A\r\n",
    "        return self.sigmoid(A)\r\n",
    "    \r\n",
    "    def backward(self, dZ):\r\n",
    "        _sig = self.sigmoid(self.A)\r\n",
    "        return dZ * (1 - _sig)*_sig\r\n",
    "    \r\n",
    "    def sigmoid(self, X):\r\n",
    "        return 1 / (1 + np.exp(-X))\r\n",
    "\r\n",
    "class Tanh:\r\n",
    "    \r\n",
    "    def forward(self, A):\r\n",
    "        self.A = A\r\n",
    "        return np.tanh(A)\r\n",
    "    \r\n",
    "    def backward(self, dZ):\r\n",
    "        return dZ * (1 - (np.tanh(self.A))**2)\r\n",
    "\r\n",
    "class Softmax:\r\n",
    "    \r\n",
    "    def forward(self, X):\r\n",
    "        self.Z = np.exp(X) / np.sum(np.exp(X), axis=1).reshape(-1,1)\r\n",
    "        return self.Z\r\n",
    "    \r\n",
    "    def backward(self, Y):\r\n",
    "        self.loss = self.loss_func(Y)\r\n",
    "        return self.Z - Y\r\n",
    "    \r\n",
    "    def loss_func(self, Y, Z=None):\r\n",
    "        if Z is None:\r\n",
    "            Z = self.Z\r\n",
    "        return (-1)*np.average(np.sum(Y*np.log(Z), axis=1))\r\n",
    "\r\n",
    "class ReLU:\r\n",
    "    \r\n",
    "    def forward(self, A):\r\n",
    "        self.A = A\r\n",
    "        return np.clip(A, 0, None)\r\n",
    "    \r\n",
    "    def backward(self, dZ):\r\n",
    "        return dZ * np.clip(np.sign(self.A), 0, None)\r\n",
    "\r\n",
    "class FC:\r\n",
    "\r\n",
    "    def __init__(self, n_nodes1, n_nodes2, initializer, optimizer):\r\n",
    "        self.optimizer = optimizer\r\n",
    "        self.W = initializer.W(n_nodes1, n_nodes2)\r\n",
    "        self.B = initializer.B(n_nodes2)\r\n",
    "        \r\n",
    "    def forward(self, X):\r\n",
    "        self.X = X\r\n",
    "        A = X@self.W + self.B\r\n",
    "        return A\r\n",
    "    \r\n",
    "    def backward(self, dA):\r\n",
    "        dZ = dA@self.W.T\r\n",
    "        self.dB = np.sum(dA, axis=0)\r\n",
    "        self.dW = self.X.T@dA\r\n",
    "        self.optimizer.update(self)\r\n",
    "        return dZ\r\n",
    "\r\n",
    "class XavierInitializer:\r\n",
    "    \r\n",
    "    def W(self, n_nodes1, n_nodes2):\r\n",
    "        self.sigma = math.sqrt(1 / n_nodes1)\r\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\r\n",
    "        return W\r\n",
    "    \r\n",
    "    def B(self, n_nodes2):\r\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\r\n",
    "        return B\r\n",
    "    \r\n",
    "class HeInitializer():\r\n",
    "    \r\n",
    "    def W(self, n_nodes1, n_nodes2):\r\n",
    "        self.sigma = math.sqrt(2 / n_nodes1)\r\n",
    "        W = self.sigma * np.random.randn(n_nodes1, n_nodes2)\r\n",
    "        return W\r\n",
    "    \r\n",
    "    def B(self, n_nodes2):\r\n",
    "        B = self.sigma * np.random.randn(n_nodes2)\r\n",
    "        return B\r\n",
    "\r\n",
    "class SGD:\r\n",
    "\r\n",
    "    def __init__(self, lr):\r\n",
    "        self.lr = lr\r\n",
    "    \r\n",
    "    def update(self, layer):\r\n",
    "        layer.W -= self.lr * layer.dW\r\n",
    "        layer.B -= self.lr * layer.dB\r\n",
    "        return\r\n",
    "\r\n",
    "class AdaGrad:\r\n",
    "    \r\n",
    "    def __init__(self, lr):\r\n",
    "        self.lr = lr\r\n",
    "        self.HW = 1\r\n",
    "        self.HB = 1\r\n",
    "    \r\n",
    "    def update(self, layer):\r\n",
    "        self.HW += layer.dW**2\r\n",
    "        self.HB += layer.dB**2\r\n",
    "        layer.W -= self.lr * np.sqrt(1/self.HW) * layer.dW\r\n",
    "        layer.B -= self.lr * np.sqrt(1/self.HB) * layer.dB\r\n",
    "\r\n",
    "class Conv1d_Arbitrary_Strides:\r\n",
    "    \r\n",
    "    def __init__(self, b_size, initializer, optimizer, n_in_channels=1, n_out_channels=1, pa=0, stride=1):\r\n",
    "        self.b_size = b_size\r\n",
    "        self.optimizer = optimizer\r\n",
    "        self.pa = pa\r\n",
    "        self.stride = stride\r\n",
    "        self.W = initializer.W(n_out_channels, n_in_channels, b_size)\r\n",
    "        self.B = initializer.B(n_out_channels)\r\n",
    "        self.n_in_channels = n_in_channels\r\n",
    "        self.n_out_channels = n_out_channels\r\n",
    "        self.n_out = None\r\n",
    "        \r\n",
    "    def forward(self, X):\r\n",
    "        \r\n",
    "        #Forward propagation while takin into account the strides\r\n",
    "        self.n_samples = X.shape[0]\r\n",
    "        self.n_in = X.shape[-1]\r\n",
    "        self.n_out = output_size_calculation(self.n_in, self.b_size, self.pa, self.stride)\r\n",
    "        X = X.reshape(self.n_samples, self.n_in_channels, self.n_in)\r\n",
    "        self.X = np.pad(X, ((0,0), (0,0), ((self.b_size-1), 0)))\r\n",
    "        self.X1 = np.zeros((self.n_samples, self.n_in_channels, self.b_size, self.n_in+(self.b_size-1)))\r\n",
    "        for i in range(self.b_size):\r\n",
    "            self.X1[:, :, i] = np.roll(self.X, -i, axis=-1)\r\n",
    "        A = np.sum(self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride]*self.W[:, :, :, np.newaxis], axis=(2, 3)) + self.B.reshape(-1,1)\r\n",
    "        return A\r\n",
    "    \r\n",
    "    def backward(self, dA):\r\n",
    "        \r\n",
    "        #Back propagation while takin into account the strides\r\n",
    "        self.dW = np.sum(dA[:, :, np.newaxis, np.newaxis]*self.X1[:, np.newaxis, :, :, self.b_size-1-self.pa:self.n_in+self.pa:self.stride], axis=(0, -1))\r\n",
    "        self.dB = np.sum(dA, axis=(0, -1))\r\n",
    "        self.dA = np.pad(dA, ((0,0), (0,0), (0, (self.b_size-1))))\r\n",
    "        self.dA1 = np.zeros((self.n_samples, self.n_out_channels, self.b_size, self.dA.shape[-1]))\r\n",
    "        for i in range(self.b_size):\r\n",
    "            self.dA1[:, :, i] = np.roll(self.dA, i, axis=-1)\r\n",
    "        dX = np.sum(self.W[:, :, :, np.newaxis]*self.dA1[:, :, np.newaxis], axis=(1,3))\r\n",
    "        self.optimizer.update(self)\r\n",
    "        return dX\r\n",
    "\r\n",
    "def output_size_calculation(n_in, F, P=0, S=1):\r\n",
    "    n_out = int((n_in + 2*P - F) / S + 1)\r\n",
    "    return n_out\r\n",
    "\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "source": [
    "class ScratchCNNClassifier:\r\n",
    "    \r\n",
    "    def __init__(self, num_epoch=10, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=200, n_output=10, verbose=True, Activater=Tanh, Optimizer=AdaGrad):\r\n",
    "        self.num_epoch = num_epoch\r\n",
    "        self.lr = lr\r\n",
    "        self.verbose = verbose  \r\n",
    "        self.batch_size = batch_size \r\n",
    "        self.n_features = n_features \r\n",
    "        self.n_nodes2 = n_nodes2 \r\n",
    "        self.n_output = n_output \r\n",
    "        self.Activater = Activater\r\n",
    "        if Activater == Sigmoid or Activater == Tanh:\r\n",
    "            self.Initializer = XavierInitializer\r\n",
    "        elif Activater == ReLU:\r\n",
    "            self.Initializer = HeInitializer\r\n",
    "        self.Optimizer = Optimizer\r\n",
    "    \r\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\r\n",
    "        self.val_enable = False\r\n",
    "        if X_val is not None:\r\n",
    "            self.val_enable = True\r\n",
    "        \r\n",
    "        #Preparing the Convolutional layer and the Fullty Connected layers\r\n",
    "        self.Conv1d_Arbitrary_Strides = Conv1d_Arbitrary_Strides(b_size=7, initializer=SimpleInitializer(0.01), optimizer=self.Optimizer(self.lr), n_in_channels=1, n_out_channels=1, pa=3, stride=2)\r\n",
    "        self.Conv1d_Arbitrary_Strides.n_out = output_size_calculation(X.shape[-1], self.Conv1d_Arbitrary_Strides.b_size, self.Conv1d_Arbitrary_Strides.pa, self.Conv1d_Arbitrary_Strides.stride)\r\n",
    "        self.activation1 = self.Activater()\r\n",
    "        self.FC2 = FC(1*self.Conv1d_Arbitrary_Strides.n_out, self.n_nodes2, self.Initializer(), self.Optimizer(self.lr))\r\n",
    "        self.activation2 = self.Activater()\r\n",
    "        self.FC3 = FC(self.n_nodes2, self.n_output, self.Initializer(), self.Optimizer(self.lr))\r\n",
    "        self.activation3 = Softmax()\r\n",
    "        \r\n",
    "        self.loss = []\r\n",
    "        self.loss_epoch = [self.activation3.loss_func(y, self.forward_propagation(X))]\r\n",
    "        \r\n",
    "        #Going through the number of epochs, doing forward and back propagations and recording the loss.\r\n",
    "        for _ in range(self.num_epoch):\r\n",
    "            #Instantiation of the GetMiniBatch class\r\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size)\r\n",
    "            self.iter = len(get_mini_batch)\r\n",
    "            for mini_X, mini_y in get_mini_batch:\r\n",
    "                self.forward_propagation(mini_X)\r\n",
    "                self.back_propagation(mini_X, mini_y)\r\n",
    "                self.loss.append(self.activation3.loss)\r\n",
    "            self.loss_epoch.append(self.activation3.loss_func(y, self.forward_propagation(X)))\r\n",
    "    \r\n",
    "    #Doing predictions. Using np.argmax() in order to get the indice of the highest value \r\n",
    "    def predict(self, X):\r\n",
    "        return np.argmax(self.forward_propagation(X), axis=1)\r\n",
    "    \r\n",
    "    def forward_propagation(self, X):\r\n",
    "        \r\n",
    "        #Forward Propagation from the COnvolutional layer to the Fully Connected layers\r\n",
    "        A1 = self.Conv1d_Arbitrary_Strides.forward(X)\r\n",
    "        A1 = A1.reshape(A1.shape[0], A1.shape[-1])\r\n",
    "        Z1 = self.activation1.forward(A1)\r\n",
    "        A2 = self.FC2.forward(Z1)\r\n",
    "        Z2 = self.activation2.forward(A2)\r\n",
    "        A3 = self.FC3.forward(Z2)\r\n",
    "        Z3 = self.activation3.forward(A3)\r\n",
    "        return Z3\r\n",
    "        \r\n",
    "    def back_propagation(self, X, y_true):\r\n",
    "        \r\n",
    "        #back Propagation from the Fully Connected layers to the Convolutional layer\r\n",
    "        dA3 = self.activation3.backward(y_true) \r\n",
    "        dZ2 = self.FC3.backward(dA3)\r\n",
    "        dA2 = self.activation2.backward(dZ2)\r\n",
    "        dZ1 = self.FC2.backward(dA2)\r\n",
    "        dA1 = self.activation1.backward(dZ1)\r\n",
    "        dA1 = dA1[:, np.newaxis]\r\n",
    "        dZ0 = self.Conv1d_Arbitrary_Strides.backward(dA1) \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\r\n",
    "\r\n",
    "X_train = X_train.reshape(-1, 784)\r\n",
    "X_test = X_test.reshape(-1, 784)\r\n",
    "\r\n",
    "\r\n",
    "X_train = X_train.astype(np.float)\r\n",
    "X_test = X_test.astype(np.float)\r\n",
    "X_train /= 255\r\n",
    "X_test /= 255\r\n",
    "\r\n",
    "enc = OneHotEncoder(handle_unknown='ignore', sparse=False)\r\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\r\n",
    "y_test_one_hot = enc.transform(y_test[:, np.newaxis])\r\n",
    "\r\n",
    "X_train_, X_val, y_train_, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2)\r\n",
    "\r\n",
    "test3 = ScratchCNNClassifier(num_epoch=20, lr=0.01, batch_size=20, n_features=784, n_nodes1=400, n_nodes2=400, n_output=10, verbose=True, Activater=Tanh, Optimizer=SGD)\r\n",
    "test3.fit(X_train_, y_train_)\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-52-8d0a3d370e29>:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_train = X_train.astype(np.float)\n",
      "<ipython-input-52-8d0a3d370e29>:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  X_test = X_test.astype(np.float)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "source": [
    "y_pred = test3.predict(X_test)\r\n",
    "accuracy_score(y_test, y_pred)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.9803"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.11 64-bit ('DIC01env': conda)"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.11",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "interpreter": {
   "hash": "6b7f8bdceed5477f092ac7c41d593afd8321a641dc4c59fe20ae9add7ebabcd8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}